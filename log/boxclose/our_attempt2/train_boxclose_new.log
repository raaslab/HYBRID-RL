=================config=================
add_bc_loss: 0
batch_size: 256
bc_policy: boxclose
discount: 0.99
env_reward_scale: 1
episode_length: 100
log_per_step: 5000
mix_rl_rate: 1
nstep: 3
num_critic_update: 1
num_eval_episode: 20
num_train_step: 60000
num_warm_up_episode: 50
preload_datapath: release/data/metaworld/BoxClose_frame_stack_1_96x96_end_on_success/dataset.hdf5
preload_num_data: 3
pretrain_epoch_len: 10000
pretrain_num_epoch: 0
q_agent:
  act_method: ibrl
  actor:
    dropout: 0.0
    feature_dim: 64
    hidden_dim: 1024
    max_action_norm: -1
    orth: 1
    spatial_emb: 0
  bc_loss_coef: 0.1
  bc_loss_dynamic: 0
  bootstrap_method: ibrl
  critic:
    drop: 0
    feature_dim: 64
    fuse_patch: 1
    hidden_dim: 1024
    norm_weight: 0
    orth: 1
    spatial_emb: 0
  critic_target_tau: 0.01
  device: cuda
  enc_type: drq
  ibrl_eps_greedy: 1
  lr: 0.0001
  resnet:
    downsample: default
    norm_layer: gnn
    shallow: 0
    stem: default
  resnet96:
    shallow: 0
    use_1x1: 0
  soft_ibrl_beta: 10
  state_actor:
    dropout: 0.5
    hidden_dim: 512
    layer_norm: 0
    num_layer: 3
    orth: 0
  state_critic:
    append_action: 0
    dropout: 0.0
    hidden_dim: 512
    layer_norm: 0
    num_k: 2
    num_layer: 3
    num_q: 10
    orth: 0
  stddev_clip: 0.3
  use_prop: 0
  vit:
    depth: 3
    embed_dim: 128
    embed_norm: 0
    embed_style: embed1
    num_heads: 4
    patch_size: 8
    stride: -1
replay_buffer_size: 500
save_dir: experiments/rl/metaworld/boxclose_new
seed: 1
stddev_max: 0.1
stddev_min: 0.1
stddev_step: 500000
update_freq: 2
use_bc: 1
use_wb: 1
========================================
=========config of loaded agent=========
batchsize: 256
dataset:
  action_repeat: 2
  eval_episode_len: 100
  frame_stack: 1
  max_len: -1
  num_data: 3
  obs_stack: 1
  path: data/metaworld/BoxClose_frame_stack_1_96x96_end_on_success/dataset.hdf5
  rl_camera: corner2
  use_state: 0
ema: -1
epoch_len: 10000
grad_clip: 5
load_model: none
lr: 0.0001
num_epoch: 2
policy:
  dropout: 0
  feature_dim: 256
  hidden_dim: 1024
  net_type: resnet
  orth_init: 1
  proj_dim: 1024
  resnet:
    downsample: default
    norm_layer: gnn
    shallow: 0
    stem: patch
  use_prop: 0
rl_image_size: 96
robot: Sawyer
save_dir: exps/bc/metaworld/pathBoxClose_num_data3_num_epoch2_seed1
seed: 1
task_name: BoxClose
use_wb: 1
weight_decay: 0
========================================
norm layer: gnn
===============Env Config===============
{'action_repeat': 2,
 'camera_names': ['corner2'],
 'device': 'cuda',
 'env_name': 'BoxClose',
 'episode_length': 100,
 'frame_stack': 1,
 'obs_stack': 1,
 'reward_shaping': False,
 'rl_camera': 'corner2',
 'rl_image_size': 96,
 'robots': 'Sawyer',
 'use_state': 0}
========================================
encoder output dim:  39200
patch output dim:  32
============encoder weights=============
DrQEncoder(
  (transform): Resize(size=84, interpolation=bicubic, max_size=None, antialias=True)
  (convnet): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))
    (1): ReLU()
    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
    (3): ReLU()
    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
)
| Module           |   #Params |      % |
|------------------+-----------+--------|
| convnet.0.weight |       864 |   3.02 |
| convnet.0.bias   |        32 |   0.11 |
| convnet.2.weight |     9,216 |  32.18 |
| convnet.2.bias   |        32 |   0.11 |
| convnet.4.weight |     9,216 |  32.18 |
| convnet.4.bias   |        32 |   0.11 |
| convnet.6.weight |     9,216 |  32.18 |
| convnet.6.bias   |        32 |   0.11 |
| Total            |    28,640 | 100.00 |
=============critic weights=============
Critic(
  (q1): _QNet(
    (obs_proj): Sequential(
      (0): Linear(in_features=39200, out_features=64, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
    )
    (q): Sequential(
      (0): Linear(in_features=68, out_features=1024, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (7): ReLU(inplace=True)
      (8): Linear(in_features=1024, out_features=1, bias=True)
    )
  )
  (q2): _QNet(
    (obs_proj): Sequential(
      (0): Linear(in_features=39200, out_features=64, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
    )
    (q): Sequential(
      (0): Linear(in_features=68, out_features=1024, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (7): ReLU(inplace=True)
      (8): Linear(in_features=1024, out_features=1, bias=True)
    )
  )
)
| Module               |   #Params |      % |
|----------------------+-----------+--------|
| q1.obs_proj.0.weight | 2,508,800 |  34.51 |
| q1.obs_proj.0.bias   |        64 |   0.00 |
| q1.obs_proj.2.weight |        64 |   0.00 |
| q1.obs_proj.2.bias   |        64 |   0.00 |
| q1.q.0.weight        |    69,632 |   0.96 |
| q1.q.0.bias          |     1,024 |   0.01 |
| q1.q.2.weight        |     1,024 |   0.01 |
| q1.q.2.bias          |     1,024 |   0.01 |
| q1.q.4.weight        | 1,048,576 |  14.43 |
| q1.q.4.bias          |     1,024 |   0.01 |
| q1.q.6.weight        |     1,024 |   0.01 |
| q1.q.6.bias          |     1,024 |   0.01 |
| q1.q.8.weight        |     1,024 |   0.01 |
| q1.q.8.bias          |         1 |   0.00 |
| q2.obs_proj.0.weight | 2,508,800 |  34.51 |
| q2.obs_proj.0.bias   |        64 |   0.00 |
| q2.obs_proj.2.weight |        64 |   0.00 |
| q2.obs_proj.2.bias   |        64 |   0.00 |
| q2.q.0.weight        |    69,632 |   0.96 |
| q2.q.0.bias          |     1,024 |   0.01 |
| q2.q.2.weight        |     1,024 |   0.01 |
| q2.q.2.bias          |     1,024 |   0.01 |
| q2.q.4.weight        | 1,048,576 |  14.43 |
| q2.q.4.bias          |     1,024 |   0.01 |
| q2.q.6.weight        |     1,024 |   0.01 |
| q2.q.6.bias          |     1,024 |   0.01 |
| q2.q.8.weight        |     1,024 |   0.01 |
| q2.q.8.bias          |         1 |   0.00 |
| Total                | 7,268,738 | 100.00 |
=============actor weights==============
Actor(
  (compress): Sequential(
    (0): Linear(in_features=39200, out_features=64, bias=True)
    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.0, inplace=False)
    (3): ReLU()
  )
  (policy): Sequential(
    (0): Linear(in_features=64, out_features=1024, bias=True)
    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.0, inplace=False)
    (3): ReLU()
    (4): Linear(in_features=1024, out_features=1024, bias=True)
    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (6): Dropout(p=0.0, inplace=False)
    (7): ReLU()
    (8): Linear(in_features=1024, out_features=4, bias=True)
    (9): Tanh()
  )
)
| Module            |   #Params |      % |
|-------------------+-----------+--------|
| compress.0.weight | 2,508,800 |  69.05 |
| compress.0.bias   |        64 |   0.00 |
| compress.1.weight |        64 |   0.00 |
| compress.1.bias   |        64 |   0.00 |
| policy.0.weight   |    65,536 |   1.80 |
| policy.0.bias     |     1,024 |   0.03 |
| policy.1.weight   |     1,024 |   0.03 |
| policy.1.bias     |     1,024 |   0.03 |
| policy.4.weight   | 1,048,576 |  28.86 |
| policy.4.bias     |     1,024 |   0.03 |
| policy.5.weight   |     1,024 |   0.03 |
| policy.5.bias     |     1,024 |   0.03 |
| policy.8.weight   |     4,096 |   0.11 |
| policy.8.bias     |         4 |   0.00 |
| Total             | 3,633,348 | 100.00 |
loading first 3 episodes from release/data/metaworld/BoxClose_frame_stack_1_96x96_end_on_success/dataset.hdf5
Raw Dataset size (#episode): 5
rewards: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
rewards: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1.]
rewards: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1.]
Size of the replay buffer: 3, # success: 3
obs torch.Size([3, 96, 96])
prop torch.Size([4])
Warm up done. #episode: 50
#episode from warmup: 47, #reward: 21.0
dict_keys(['obs', 'prop'])
dict_keys(['corner2'])
Saved model to experiments/rl/metaworld/boxclose_new/model0.pt
saved?: True
[5000] Time spent = 603.04 s
5000: other/elapsed_time  : 499.07
5000: other/episode       : 51
5000: other/replay        : 101
5000: other/speed         : 10.02
5000: other/step          : 5000
5000: other/train_step    : 2499
5000: score/num_success   : 27
5000: score/score         : 0.10
5000: actor/anorm_bc      [ 4768]: avg:   1.1293, min:   0.5189[ 688], max:   1.8161[4759]
5000: actor/anorm_rl      [ 4768]: avg:   1.4958, min:   0.2086[4512], max:   2.0000[  25]
5000: actor/bc_eval       [ 1949]: avg:   0.3751, min:   0.0000[   1], max:   1.0000[  20]
5000: actor/bc_train      [ 4768]: avg:   0.3314, min:   0.0000[   9], max:   1.0000[   1]
5000: actor/bootstrap_bc  [ 2499]: avg:   0.1833, min:   0.0000[  57], max:   0.9141[  25]
5000: data/batch_R        [ 2499]: avg:   0.0117, min:   0.0000[  26], max:   0.0387[ 412]
5000: data/discount       [ 2499]: avg:   0.9366, min:   0.8907[ 670], max:   0.9665[ 138]
5000: data/episode_len    [   51]: avg:  97.0588, min:  45.0000[  51], max: 100.0000[   1]
5000: data/stddev         [ 4768]: avg:   0.1000, min:   0.1000[  15], max:   0.1000[   2]
5000: score/train_score   [   51]: avg:   0.0588, min:   0.0000[   1], max:   1.0000[  28]
5000: train/actor_loss    [ 2499]: avg:  -0.1925, min:  -0.5630[2494], max:   3.0245[   1]
5000: train/critic_loss   [ 2499]: avg:   0.0260, min:   0.0047[ 370], max:  13.3788[   2]
5000: train/critic_qt     [ 2499]: avg:   0.1512, min:  -0.8805[  13], max:   0.5211[2485]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| env step |  5000 |          11.2 |  11.8 |
| add      |  5000 |           0.1 |   0.1 |
| train    |  2499 |         123.5 |  64.8 |
| act      |  4768 |          15.5 |  15.6 |
| reset    |    51 |          47.1 |   0.5 |
| eval     |     1 |       34221.3 |   7.2 |
| total(s) |     1 |         475.9 | 100   |
total time: 0:08:53
Mem info: used: 6.911 GB, avail: 122.854 GB, total: 156.060 GB
saved?: False
[10000] Time spent = 533.00 s
10000: other/elapsed_time  : 495.35
10000: other/episode       : 116
10000: other/replay        : 166
10000: other/speed         : 10.09
10000: other/step          : 10000
10000: other/train_step    : 4999
10000: score/num_success   : 57
10000: score/score         : 0.05
10000: actor/anorm_bc      [ 4680]: avg:   1.2720, min:   0.5381[ 552], max:   1.8336[1634]
10000: actor/anorm_rl      [ 4680]: avg:   1.2802, min:   0.0300[1934], max:   2.0000[ 335]
10000: actor/bc_eval       [ 1973]: avg:   0.2808, min:   0.0000[   1], max:   1.0000[  53]
10000: actor/bc_train      [ 4680]: avg:   0.3256, min:   0.0000[   1], max:   1.0000[   3]
10000: actor/bootstrap_bc  [ 2500]: avg:   0.1847, min:   0.1016[2424], max:   0.2852[1464]
10000: data/batch_R        [ 2500]: avg:   0.0095, min:   0.0000[  17], max:   0.0349[2217]
10000: data/discount       [ 2500]: avg:   0.9371, min:   0.8983[2280], max:   0.9703[ 767]
10000: data/episode_len    [   65]: avg:  77.2000, min:  30.0000[  54], max: 100.0000[   1]
10000: data/stddev         [ 4680]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[   2]
10000: score/train_score   [   65]: avg:   0.4615, min:   0.0000[   1], max:   1.0000[   4]
10000: train/actor_loss    [ 2500]: avg:  -0.4569, min:  -0.5671[ 603], max:  -0.3634[1674]
10000: train/critic_loss   [ 2500]: avg:   0.0097, min:   0.0034[1361], max:   0.0248[ 190]
10000: train/critic_qt     [ 2500]: avg:   0.4446, min:   0.3807[2293], max:   0.5137[ 273]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         122.1 |  64.2 |
| act      |  4680 |          15.3 |  15.1 |
| env step |  5000 |          11.5 |  12   |
| add      |  5000 |           0.2 |   0.2 |
| reset    |    65 |          46.2 |   0.6 |
| eval     |     1 |       37430.4 |   7.9 |
| total(s) |     1 |         475.4 | 100   |
total time: 0:17:46
Mem info: used: 6.911 GB, avail: 122.809 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/boxclose_new/model0.pt
saved?: True
[15000] Time spent = 531.95 s
15000: other/elapsed_time  : 497.12
15000: other/episode       : 197
15000: other/replay        : 247
15000: other/speed         : 10.06
15000: other/step          : 15000
15000: other/train_step    : 7499
15000: score/num_success   : 110
15000: score/score         : 0.15
15000: actor/anorm_bc      [ 4725]: avg:   1.2890, min:   0.5914[ 224], max:   1.8282[ 296]
15000: actor/anorm_rl      [ 4725]: avg:   1.4404, min:   0.2557[ 732], max:   2.0000[1266]
15000: actor/bc_eval       [ 1847]: avg:   0.2994, min:   0.0000[   1], max:   1.0000[  12]
15000: actor/bc_train      [ 4725]: avg:   0.2897, min:   0.0000[   1], max:   1.0000[   2]
15000: actor/bootstrap_bc  [ 2500]: avg:   0.1527, min:   0.0781[1200], max:   0.2461[ 388]
15000: data/batch_R        [ 2500]: avg:   0.0144, min:   0.0000[  11], max:   0.0426[2081]
15000: data/discount       [ 2500]: avg:   0.9345, min:   0.8869[2443], max:   0.9665[1255]
15000: data/episode_len    [   81]: avg:  61.9753, min:  30.0000[  11], max: 100.0000[   1]
15000: data/stddev         [ 4725]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[   4]
15000: score/train_score   [   81]: avg:   0.6543, min:   0.0000[   1], max:   1.0000[   3]
15000: train/actor_loss    [ 2500]: avg:  -0.4640, min:  -0.5353[2081], max:  -0.3854[ 905]
15000: train/critic_loss   [ 2500]: avg:   0.0087, min:   0.0034[2190], max:   0.0214[1885]
15000: train/critic_qt     [ 2500]: avg:   0.4512, min:   0.3978[ 458], max:   0.5130[2373]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         121.6 |  64.1 |
| act      |  4725 |          15.5 |  15.5 |
| env step |  5000 |          11.5 |  12.1 |
| add      |  5000 |           0.2 |   0.2 |
| reset    |    81 |          47.6 |   0.8 |
| eval     |     1 |       34459.7 |   7.3 |
| total(s) |     1 |         474.1 | 100   |
total time: 0:26:38
Mem info: used: 6.911 GB, avail: 122.810 GB, total: 156.060 GB
saved?: False
[20000] Time spent = 533.41 s
20000: other/elapsed_time  : 497.98
20000: other/episode       : 285
20000: other/replay        : 335
20000: other/speed         : 10.04
20000: other/step          : 20000
20000: other/train_step    : 9999
20000: score/num_success   : 173
20000: score/score         : 0.05
20000: actor/anorm_bc      [ 4679]: avg:   1.2898, min:   0.5804[3955], max:   1.8304[4532]
20000: actor/anorm_rl      [ 4679]: avg:   1.4712, min:   0.1687[3527], max:   2.0000[  46]
20000: actor/bc_eval       [ 1931]: avg:   0.3744, min:   0.0000[   1], max:   1.0000[  12]
20000: actor/bc_train      [ 4679]: avg:   0.2058, min:   0.0000[   1], max:   1.0000[   6]
20000: actor/bootstrap_bc  [ 2500]: avg:   0.1389, min:   0.0664[1764], max:   0.2227[ 170]
20000: data/batch_R        [ 2500]: avg:   0.0187, min:   0.0000[  63], max:   0.0619[2496]
20000: data/discount       [ 2500]: avg:   0.9315, min:   0.8869[2447], max:   0.9627[ 305]
20000: data/episode_len    [   88]: avg:  56.7045, min:  25.0000[   3], max: 100.0000[   2]
20000: data/stddev         [ 4679]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[   4]
20000: score/train_score   [   88]: avg:   0.7159, min:   0.0000[   2], max:   1.0000[   1]
20000: train/actor_loss    [ 2500]: avg:  -0.4920, min:  -0.5769[1119], max:  -0.4154[ 700]
20000: train/critic_loss   [ 2500]: avg:   0.0090, min:   0.0035[1252], max:   0.0209[ 809]
20000: train/critic_qt     [ 2500]: avg:   0.4792, min:   0.4302[ 130], max:   0.5451[1119]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         121.2 |  63.9 |
| act      |  4679 |          15.6 |  15.4 |
| env step |  5000 |          11.7 |  12.3 |
| add      |  5000 |           0.2 |   0.2 |
| reset    |    88 |          46   |   0.9 |
| eval     |     1 |       35245.2 |   7.4 |
| total(s) |     1 |         474.4 | 100   |
total time: 0:35:31
Mem info: used: 6.911 GB, avail: 122.769 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/boxclose_new/model0.pt
saved?: True
[25000] Time spent = 524.44 s
25000: other/elapsed_time  : 495.91
25000: other/episode       : 393
25000: other/replay        : 443
25000: other/speed         : 10.08
25000: other/step          : 25000
25000: other/train_step    : 12499
25000: score/num_success   : 261
25000: score/score         : 0.35
25000: actor/anorm_bc      [ 4612]: avg:   1.3000, min:   0.6259[1651], max:   1.8158[1251]
25000: actor/anorm_rl      [ 4612]: avg:   1.5182, min:   0.3310[ 634], max:   2.0000[3313]
25000: actor/bc_eval       [ 1521]: avg:   0.1887, min:   0.0000[   1], max:   1.0000[  12]
25000: actor/bc_train      [ 4612]: avg:   0.2127, min:   0.0000[   1], max:   1.0000[  28]
25000: actor/bootstrap_bc  [ 2500]: avg:   0.1124, min:   0.0469[1947], max:   0.2070[ 293]
25000: data/batch_R        [ 2500]: avg:   0.0236, min:   0.0000[1259], max:   0.0581[1946]
25000: data/discount       [ 2500]: avg:   0.9279, min:   0.8718[1494], max:   0.9627[ 235]
25000: data/episode_len    [  108]: avg:  46.2407, min:  28.0000[  24], max: 100.0000[   1]
25000: data/stddev         [ 4612]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[  11]
25000: score/train_score   [  108]: avg:   0.8148, min:   0.0000[   1], max:   1.0000[   2]
25000: train/actor_loss    [ 2500]: avg:  -0.5264, min:  -0.6181[2397], max:  -0.4286[ 322]
25000: train/critic_loss   [ 2500]: avg:   0.0089, min:   0.0039[ 773], max:   0.0247[1878]
25000: train/critic_qt     [ 2500]: avg:   0.5118, min:   0.4412[ 322], max:   0.5959[2397]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         121.5 |  65.2 |
| act      |  4612 |          15.4 |  15.3 |
| env step |  5000 |          11.3 |  12.2 |
| add      |  5000 |           0.2 |   0.2 |
| reset    |   108 |          46.9 |   1.1 |
| eval     |     1 |       28138.6 |   6   |
| total(s) |     1 |         465.6 | 100   |
total time: 0:44:16
Mem info: used: 6.901 GB, avail: 122.742 GB, total: 156.060 GB
saved?: False
[30000] Time spent = 530.19 s
30000: other/elapsed_time  : 498.98
30000: other/episode       : 522
30000: other/replay        : 500
30000: other/speed         : 10.02
30000: other/step          : 30000
30000: other/train_step    : 14999
30000: score/num_success   : 380
30000: score/score         : 0.30
30000: actor/anorm_bc      [ 4502]: avg:   1.3140, min:   0.5565[ 886], max:   1.8310[3547]
30000: actor/anorm_rl      [ 4502]: avg:   1.4943, min:   0.2723[1606], max:   2.0000[ 278]
30000: actor/bc_eval       [ 1589]: avg:   0.2379, min:   0.0000[   1], max:   1.0000[  15]
30000: actor/bc_train      [ 4502]: avg:   0.1588, min:   0.0000[   1], max:   1.0000[  11]
30000: actor/bootstrap_bc  [ 2500]: avg:   0.1013, min:   0.0469[1109], max:   0.1797[ 278]
30000: data/batch_R        [ 2500]: avg:   0.0305, min:   0.0038[ 124], max:   0.0735[2311]
30000: data/discount       [ 2500]: avg:   0.9228, min:   0.8755[1778], max:   0.9589[ 321]
30000: data/episode_len    [  129]: avg:  38.8062, min:  27.0000[  79], max: 100.0000[  13]
30000: data/stddev         [ 4502]: avg:   0.1000, min:   0.1000[  23], max:   0.1000[   8]
30000: score/train_score   [  129]: avg:   0.9225, min:   0.0000[  13], max:   1.0000[   1]
30000: train/actor_loss    [ 2500]: avg:  -0.5849, min:  -0.6703[2429], max:  -0.5116[ 885]
30000: train/critic_loss   [ 2500]: avg:   0.0080, min:   0.0033[1146], max:   0.0197[2458]
30000: train/critic_qt     [ 2500]: avg:   0.5702, min:   0.5138[  12], max:   0.6518[2429]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         121.7 |  65   |
| act      |  4502 |          15.6 |  15   |
| env step |  5000 |          11.2 |  12   |
| add      |  5000 |           0.2 |   0.2 |
| reset    |   129 |          47.3 |   1.3 |
| eval     |     1 |       31012.4 |   6.6 |
| total(s) |     1 |         468.5 | 100   |
total time: 0:53:06
Mem info: used: 6.901 GB, avail: 122.715 GB, total: 156.060 GB
saved?: False
[35000] Time spent = 526.66 s
35000: other/elapsed_time  : 499.49
35000: other/episode       : 663
35000: other/replay        : 500
35000: other/speed         : 10.01
35000: other/step          : 35000
35000: other/train_step    : 17499
35000: score/num_success   : 514
35000: score/score         : 0.20
35000: actor/anorm_bc      [ 4417]: avg:   1.3396, min:   0.6770[3158], max:   1.8352[4140]
35000: actor/anorm_rl      [ 4417]: avg:   1.4834, min:   0.2452[2332], max:   2.0000[ 546]
35000: actor/bc_eval       [ 1734]: avg:   0.3241, min:   0.0000[   1], max:   1.0000[   9]
35000: actor/bc_train      [ 4417]: avg:   0.1519, min:   0.0000[   2], max:   1.0000[   1]
35000: actor/bootstrap_bc  [ 2500]: avg:   0.0990, min:   0.0391[1619], max:   0.1680[1649]
35000: data/batch_R        [ 2500]: avg:   0.0498, min:   0.0077[ 277], max:   0.1045[2371]
35000: data/discount       [ 2500]: avg:   0.9104, min:   0.8566[2286], max:   0.9551[ 756]
35000: data/episode_len    [  141]: avg:  35.1489, min:  28.0000[  27], max: 100.0000[  46]
35000: data/stddev         [ 4417]: avg:   0.1000, min:   0.1000[  13], max:   0.1000[   3]
35000: score/train_score   [  141]: avg:   0.9504, min:   0.0000[  46], max:   1.0000[   1]
35000: train/actor_loss    [ 2500]: avg:  -0.6807, min:  -0.7505[1931], max:  -0.5908[ 165]
35000: train/critic_loss   [ 2500]: avg:   0.0056, min:   0.0021[1899], max:   0.0173[ 754]
35000: train/critic_qt     [ 2500]: avg:   0.6700, min:   0.5738[   9], max:   0.7409[2105]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         121.6 |  65.5 |
| act      |  4417 |          15.6 |  14.8 |
| env step |  5000 |          11.4 |  12.3 |
| add      |  5000 |           0.1 |   0.2 |
| reset    |   141 |          47.7 |   1.4 |
| eval     |     1 |       26990.4 |   5.8 |
| total(s) |     1 |         464.5 | 100   |
total time: 1:01:53
Mem info: used: 6.901 GB, avail: 128.882 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/boxclose_new/model0.pt
saved?: True
[40000] Time spent = 379.64 s
40000: other/elapsed_time  : 361.35
40000: other/episode       : 824
40000: other/replay        : 500
40000: other/speed         : 13.84
40000: other/step          : 40000
40000: other/train_step    : 19999
40000: score/num_success   : 672
40000: score/score         : 0.45
40000: actor/anorm_bc      [ 4456]: avg:   1.3555, min:   0.5932[1528], max:   1.8334[1918]
40000: actor/anorm_rl      [ 4456]: avg:   1.5270, min:   0.5022[1517], max:   2.0000[ 125]
40000: actor/bc_eval       [ 1366]: avg:   0.2042, min:   0.0000[   1], max:   1.0000[  12]
40000: actor/bc_train      [ 4456]: avg:   0.1654, min:   0.0000[   1], max:   1.0000[   3]
40000: actor/bootstrap_bc  [ 2500]: avg:   0.1114, min:   0.0352[1193], max:   0.1914[2224]
40000: data/batch_R        [ 2500]: avg:   0.0676, min:   0.0194[  56], max:   0.1236[ 828]
40000: data/discount       [ 2500]: avg:   0.8967, min:   0.8376[ 828], max:   0.9476[ 651]
40000: data/episode_len    [  161]: avg:  31.3106, min:  26.0000[ 124], max: 100.0000[   1]
40000: data/stddev         [ 4456]: avg:   0.1000, min:   0.1000[  16], max:   0.1000[   3]
40000: score/train_score   [  161]: avg:   0.9814, min:   0.0000[   1], max:   1.0000[   2]
40000: train/actor_loss    [ 2500]: avg:  -0.7418, min:  -0.8276[2384], max:  -0.6467[ 256]
40000: train/critic_loss   [ 2500]: avg:   0.0034, min:   0.0007[2478], max:   0.0113[1426]
40000: train/critic_qt     [ 2500]: avg:   0.7356, min:   0.6477[ 256], max:   0.8147[2395]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |          91.9 |  69.2 |
| act      |  4456 |           9.8 |  13.2 |
| env step |  5000 |           6.7 |  10.2 |
| add      |  5000 |           0.1 |   0.2 |
| reset    |   161 |          38.4 |   1.9 |
| eval     |     1 |       17934.4 |   5.4 |
| total(s) |     1 |         332.1 | 100   |
total time: 1:08:12
Mem info: used: 6.901 GB, avail: 128.895 GB, total: 156.060 GB
saved?: False
[45000] Time spent = 256.40 s
45000: other/elapsed_time  : 241.81
45000: other/episode       : 971
45000: other/replay        : 501
45000: other/speed         : 20.68
45000: other/step          : 45000
45000: other/train_step    : 22499
45000: score/num_success   : 811
45000: score/score         : 0.05
45000: actor/anorm_bc      [ 4508]: avg:   1.3516, min:   0.6707[3279], max:   1.8346[ 653]
45000: actor/anorm_rl      [ 4508]: avg:   1.5416, min:   0.1619[ 527], max:   2.0000[  46]
45000: actor/bc_eval       [ 1944]: avg:   0.4444, min:   0.0000[   1], max:   1.0000[   7]
45000: actor/bc_train      [ 4508]: avg:   0.1673, min:   0.0000[   1], max:   1.0000[  11]
45000: actor/bootstrap_bc  [ 2500]: avg:   0.1413, min:   0.0625[  12], max:   0.2109[ 411]
45000: data/batch_R        [ 2500]: avg:   0.0827, min:   0.0272[  17], max:   0.1429[1214]
45000: data/discount       [ 2500]: avg:   0.8855, min:   0.8301[ 243], max:   0.9438[  17]
45000: data/episode_len    [  147]: avg:  34.1769, min:  26.0000[  14], max: 100.0000[  19]
45000: data/stddev         [ 4508]: avg:   0.1000, min:   0.1000[  14], max:   0.1000[   3]
45000: score/train_score   [  147]: avg:   0.9456, min:   0.0000[  19], max:   1.0000[   1]
45000: train/actor_loss    [ 2500]: avg:  -0.8018, min:  -0.8445[2049], max:  -0.7557[1242]
45000: train/critic_loss   [ 2500]: avg:   0.0017, min:   0.0005[2283], max:   0.0066[2325]
45000: train/critic_qt     [ 2500]: avg:   0.7988, min:   0.7528[ 774], max:   0.8374[1958]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |          63.2 |  72.1 |
| act      |  4508 |           5.6 |  11.5 |
| env step |  5000 |           3.2 |   7.3 |
| add      |  5000 |           0.1 |   0.3 |
| reset    |   147 |          32.7 |   2.2 |
| eval     |     1 |       14409.5 |   6.6 |
| total(s) |     1 |         219.2 | 100   |
total time: 1:12:29
Mem info: used: 6.901 GB, avail: 135.043 GB, total: 156.060 GB
saved?: False
[50000] Time spent = 253.25 s
50000: other/elapsed_time  : 240.00
50000: other/episode       : 1127
50000: other/replay        : 500
50000: other/speed         : 20.83
50000: other/step          : 50000
50000: other/train_step    : 24999
50000: score/num_success   : 959
50000: score/score         : 0.15
50000: actor/anorm_bc      [ 4481]: avg:   1.3577, min:   0.5651[4341], max:   1.8350[4068]
50000: actor/anorm_rl      [ 4481]: avg:   1.5462, min:   0.3142[2710], max:   2.0000[  99]
50000: actor/bc_eval       [ 1784]: avg:   0.2186, min:   0.0000[   1], max:   1.0000[  12]
50000: actor/bc_train      [ 4481]: avg:   0.1966, min:   0.0000[   1], max:   1.0000[  20]
50000: actor/bootstrap_bc  [ 2500]: avg:   0.1481, min:   0.0625[ 165], max:   0.2500[ 480]
50000: data/batch_R        [ 2500]: avg:   0.0859, min:   0.0270[ 974], max:   0.1472[ 492]
50000: data/discount       [ 2500]: avg:   0.8823, min:   0.8187[2483], max:   0.9400[ 974]
50000: data/episode_len    [  156]: avg:  32.0385, min:  25.0000[  63], max: 100.0000[   5]
50000: data/stddev         [ 4481]: avg:   0.1000, min:   0.1000[   4], max:   0.1000[   1]
50000: score/train_score   [  156]: avg:   0.9487, min:   0.0000[   5], max:   1.0000[   1]
50000: train/actor_loss    [ 2500]: avg:  -0.8037, min:  -0.8443[ 219], max:  -0.7551[1527]
50000: train/critic_loss   [ 2500]: avg:   0.0014, min:   0.0003[1036], max:   0.0065[ 352]
50000: train/critic_qt     [ 2500]: avg:   0.8013, min:   0.7509[1527], max:   0.8403[ 813]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |          62.7 |  72.5 |
| env step |  5000 |           3.1 |   7.2 |
| add      |  5000 |           0.1 |   0.3 |
| act      |  4481 |           5.6 |  11.6 |
| reset    |   156 |          32.5 |   2.3 |
| eval     |     1 |       13064.6 |   6   |
| total(s) |     1 |         216   | 100   |
total time: 1:16:42
Mem info: used: 6.901 GB, avail: 135.036 GB, total: 156.060 GB
saved?: False
[55000] Time spent = 250.24 s
55000: other/elapsed_time  : 237.66
55000: other/episode       : 1298
55000: other/replay        : 500
55000: other/speed         : 21.04
55000: other/step          : 55000
55000: other/train_step    : 27499
55000: score/num_success   : 1128
55000: score/score         : 0.20
55000: actor/anorm_bc      [ 4426]: avg:   1.3678, min:   0.6968[2479], max:   1.8392[4115]
55000: actor/anorm_rl      [ 4426]: avg:   1.5481, min:   0.3625[3373], max:   2.0000[ 194]
55000: actor/bc_eval       [ 1738]: avg:   0.1536, min:   0.0000[   1], max:   1.0000[   9]
55000: actor/bc_train      [ 4426]: avg:   0.1699, min:   0.0000[   1], max:   1.0000[  21]
55000: actor/bootstrap_bc  [ 2500]: avg:   0.1294, min:   0.0664[ 222], max:   0.2266[1786]
55000: data/batch_R        [ 2500]: avg:   0.0897, min:   0.0348[1992], max:   0.1510[1571]
55000: data/discount       [ 2500]: avg:   0.8791, min:   0.8149[1461], max:   0.9324[1992]
55000: data/episode_len    [  171]: avg:  29.1228, min:  24.0000[  16], max: 100.0000[ 103]
55000: data/stddev         [ 4426]: avg:   0.1000, min:   0.1000[   6], max:   0.1000[  10]
55000: score/train_score   [  171]: avg:   0.9883, min:   0.0000[ 103], max:   1.0000[   1]
55000: train/actor_loss    [ 2500]: avg:  -0.8135, min:  -0.8525[2270], max:  -0.7626[ 299]
55000: train/critic_loss   [ 2500]: avg:   0.0010, min:   0.0002[2184], max:   0.0051[1967]
55000: train/critic_qt     [ 2500]: avg:   0.8108, min:   0.7611[ 299], max:   0.8467[ 914]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |          61.7 |  72.5 |
| act      |  4426 |           5.5 |  11.5 |
| env step |  5000 |           3.1 |   7.4 |
| add      |  5000 |           0.1 |   0.2 |
| reset    |   171 |          32.7 |   2.6 |
| eval     |     1 |       12388.9 |   5.8 |
| total(s) |     1 |         212.9 | 100   |
total time: 1:20:52
Mem info: used: 6.901 GB, avail: 135.015 GB, total: 156.060 GB
saved?: False
[60000] Time spent = 247.69 s
60000: other/elapsed_time  : 236.01
60000: other/episode       : 1444
60000: other/replay        : 500
60000: other/speed         : 21.19
60000: other/step          : 60000
60000: other/train_step    : 29999
60000: score/num_success   : 1263
60000: score/score         : 0.35
60000: actor/anorm_bc      [ 4481]: avg:   1.3157, min:   0.6363[1271], max:   1.8361[1809]
60000: actor/anorm_rl      [ 4481]: avg:   1.5386, min:   0.3518[4083], max:   2.0000[ 915]
60000: actor/bc_eval       [ 1607]: avg:   0.2526, min:   0.0000[   1], max:   1.0000[   6]
60000: actor/bc_train      [ 4481]: avg:   0.1620, min:   0.0000[   1], max:   1.0000[   3]
60000: actor/bootstrap_bc  [ 2500]: avg:   0.1251, min:   0.0391[ 955], max:   0.2031[ 247]
60000: data/batch_R        [ 2500]: avg:   0.0888, min:   0.0272[2295], max:   0.1589[2218]
60000: data/discount       [ 2500]: avg:   0.8793, min:   0.8111[2218], max:   0.9400[2295]
60000: data/episode_len    [  146]: avg:  34.3151, min:  24.0000[  94], max: 100.0000[   8]
60000: data/stddev         [ 4481]: avg:   0.1000, min:   0.1000[  48], max:   0.1000[   7]
60000: score/train_score   [  146]: avg:   0.9247, min:   0.0000[   8], max:   1.0000[   1]
60000: train/actor_loss    [ 2500]: avg:  -0.8108, min:  -0.8559[ 506], max:  -0.7647[1195]
60000: train/critic_loss   [ 2500]: avg:   0.0015, min:   0.0003[  87], max:   0.0066[ 542]
60000: train/critic_qt     [ 2500]: avg:   0.8074, min:   0.7587[1378], max:   0.8466[2024]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |          61.8 |  73.2 |
| act      |  4481 |           5.5 |  11.6 |
| env step |  5000 |           3.1 |   7.3 |
| add      |  5000 |           0.1 |   0.2 |
| reset    |   146 |          32.7 |   2.3 |
| eval     |     1 |       11506   |   5.4 |
| total(s) |     1 |         211.1 | 100   |
total time: 1:25:00
Mem info: used: 6.901 GB, avail: 134.974 GB, total: 156.060 GB
