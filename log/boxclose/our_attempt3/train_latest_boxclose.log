=================config=================
add_bc_loss: 0
batch_size: 256
bc_policy: boxclose
discount: 0.99
env_reward_scale: 1
episode_length: 100
log_per_step: 5000
mix_rl_rate: 1
nstep: 3
num_critic_update: 1
num_eval_episode: 20
num_train_step: 60000
num_warm_up_episode: 50
preload_datapath: release/data/metaworld/BoxClose_frame_stack_1_96x96_end_on_success/dataset.hdf5
preload_num_data: 3
pretrain_epoch_len: 10000
pretrain_num_epoch: 0
q_agent:
  act_method: ibrl
  actor:
    dropout: 0.0
    feature_dim: 64
    hidden_dim: 1024
    max_action_norm: -1
    orth: 1
    spatial_emb: 0
  bc_loss_coef: 0.1
  bc_loss_dynamic: 0
  bootstrap_method: ibrl
  critic:
    drop: 0
    feature_dim: 64
    fuse_patch: 1
    hidden_dim: 1024
    norm_weight: 0
    orth: 1
    spatial_emb: 0
  critic_target_tau: 0.01
  device: cuda
  enc_type: drq
  ibrl_eps_greedy: 1
  lr: 0.0001
  resnet:
    downsample: default
    norm_layer: gnn
    shallow: 0
    stem: default
  resnet96:
    shallow: 0
    use_1x1: 0
  soft_ibrl_beta: 10
  state_actor:
    dropout: 0.5
    hidden_dim: 512
    layer_norm: 0
    num_layer: 3
    orth: 0
  state_critic:
    append_action: 0
    dropout: 0.0
    hidden_dim: 512
    layer_norm: 0
    num_k: 2
    num_layer: 3
    num_q: 10
    orth: 0
  stddev_clip: 0.3
  use_prop: 0
  vit:
    depth: 3
    embed_dim: 128
    embed_norm: 0
    embed_style: embed1
    num_heads: 4
    patch_size: 8
    stride: -1
replay_buffer_size: 500
save_dir: experiments/rl/metaworld/latest_mode_waypoint_boxclose
seed: 1
stddev_max: 0.1
stddev_min: 0.1
stddev_step: 500000
update_freq: 2
use_bc: 1
use_wb: 1
========================================
=========config of loaded agent=========
batchsize: 256
dataset:
  action_repeat: 2
  eval_episode_len: 100
  frame_stack: 1
  max_len: -1
  num_data: 3
  obs_stack: 1
  path: data/metaworld/BoxClose_frame_stack_1_96x96_end_on_success/dataset.hdf5
  rl_camera: corner2
  use_state: 0
ema: -1
epoch_len: 10000
grad_clip: 5
load_model: none
lr: 0.0001
num_epoch: 2
policy:
  dropout: 0
  feature_dim: 256
  hidden_dim: 1024
  net_type: resnet
  orth_init: 1
  proj_dim: 1024
  resnet:
    downsample: default
    norm_layer: gnn
    shallow: 0
    stem: patch
  use_prop: 0
rl_image_size: 96
robot: Sawyer
save_dir: exps/bc/metaworld/pathBoxClose_num_data3_num_epoch2_seed1
seed: 1
task_name: BoxClose
use_wb: 1
weight_decay: 0
========================================
norm layer: gnn
===============Env Config===============
{'action_repeat': 2,
 'camera_names': ['corner2'],
 'device': 'cuda',
 'env_name': 'BoxClose',
 'episode_length': 100,
 'frame_stack': 1,
 'obs_stack': 1,
 'reward_shaping': False,
 'rl_camera': 'corner2',
 'rl_image_size': 96,
 'robots': 'Sawyer',
 'use_state': 0}
========================================
encoder output dim:  39200
patch output dim:  32
============encoder weights=============
DrQEncoder(
  (transform): Resize(size=84, interpolation=bicubic, max_size=None, antialias=True)
  (convnet): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))
    (1): ReLU()
    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
    (3): ReLU()
    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
)
| Module           |   #Params |      % |
|------------------+-----------+--------|
| convnet.0.weight |       864 |   3.02 |
| convnet.0.bias   |        32 |   0.11 |
| convnet.2.weight |     9,216 |  32.18 |
| convnet.2.bias   |        32 |   0.11 |
| convnet.4.weight |     9,216 |  32.18 |
| convnet.4.bias   |        32 |   0.11 |
| convnet.6.weight |     9,216 |  32.18 |
| convnet.6.bias   |        32 |   0.11 |
| Total            |    28,640 | 100.00 |
=============critic weights=============
Critic(
  (q1): _QNet(
    (obs_proj): Sequential(
      (0): Linear(in_features=39200, out_features=64, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
    )
    (q): Sequential(
      (0): Linear(in_features=68, out_features=1024, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (7): ReLU(inplace=True)
      (8): Linear(in_features=1024, out_features=1, bias=True)
    )
  )
  (q2): _QNet(
    (obs_proj): Sequential(
      (0): Linear(in_features=39200, out_features=64, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
    )
    (q): Sequential(
      (0): Linear(in_features=68, out_features=1024, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (7): ReLU(inplace=True)
      (8): Linear(in_features=1024, out_features=1, bias=True)
    )
  )
)
| Module               |   #Params |      % |
|----------------------+-----------+--------|
| q1.obs_proj.0.weight | 2,508,800 |  34.51 |
| q1.obs_proj.0.bias   |        64 |   0.00 |
| q1.obs_proj.2.weight |        64 |   0.00 |
| q1.obs_proj.2.bias   |        64 |   0.00 |
| q1.q.0.weight        |    69,632 |   0.96 |
| q1.q.0.bias          |     1,024 |   0.01 |
| q1.q.2.weight        |     1,024 |   0.01 |
| q1.q.2.bias          |     1,024 |   0.01 |
| q1.q.4.weight        | 1,048,576 |  14.43 |
| q1.q.4.bias          |     1,024 |   0.01 |
| q1.q.6.weight        |     1,024 |   0.01 |
| q1.q.6.bias          |     1,024 |   0.01 |
| q1.q.8.weight        |     1,024 |   0.01 |
| q1.q.8.bias          |         1 |   0.00 |
| q2.obs_proj.0.weight | 2,508,800 |  34.51 |
| q2.obs_proj.0.bias   |        64 |   0.00 |
| q2.obs_proj.2.weight |        64 |   0.00 |
| q2.obs_proj.2.bias   |        64 |   0.00 |
| q2.q.0.weight        |    69,632 |   0.96 |
| q2.q.0.bias          |     1,024 |   0.01 |
| q2.q.2.weight        |     1,024 |   0.01 |
| q2.q.2.bias          |     1,024 |   0.01 |
| q2.q.4.weight        | 1,048,576 |  14.43 |
| q2.q.4.bias          |     1,024 |   0.01 |
| q2.q.6.weight        |     1,024 |   0.01 |
| q2.q.6.bias          |     1,024 |   0.01 |
| q2.q.8.weight        |     1,024 |   0.01 |
| q2.q.8.bias          |         1 |   0.00 |
| Total                | 7,268,738 | 100.00 |
=============actor weights==============
Actor(
  (compress): Sequential(
    (0): Linear(in_features=39200, out_features=64, bias=True)
    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.0, inplace=False)
    (3): ReLU()
  )
  (policy): Sequential(
    (0): Linear(in_features=64, out_features=1024, bias=True)
    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.0, inplace=False)
    (3): ReLU()
    (4): Linear(in_features=1024, out_features=1024, bias=True)
    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (6): Dropout(p=0.0, inplace=False)
    (7): ReLU()
    (8): Linear(in_features=1024, out_features=4, bias=True)
    (9): Tanh()
  )
)
| Module            |   #Params |      % |
|-------------------+-----------+--------|
| compress.0.weight | 2,508,800 |  69.05 |
| compress.0.bias   |        64 |   0.00 |
| compress.1.weight |        64 |   0.00 |
| compress.1.bias   |        64 |   0.00 |
| policy.0.weight   |    65,536 |   1.80 |
| policy.0.bias     |     1,024 |   0.03 |
| policy.1.weight   |     1,024 |   0.03 |
| policy.1.bias     |     1,024 |   0.03 |
| policy.4.weight   | 1,048,576 |  28.86 |
| policy.4.bias     |     1,024 |   0.03 |
| policy.5.weight   |     1,024 |   0.03 |
| policy.5.bias     |     1,024 |   0.03 |
| policy.8.weight   |     4,096 |   0.11 |
| policy.8.bias     |         4 |   0.00 |
| Total             | 3,633,348 | 100.00 |
loading first 3 episodes from release/data/metaworld/BoxClose_frame_stack_1_96x96_end_on_success/dataset.hdf5
Raw Dataset size (#episode): 5
rewards: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
rewards: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1.]
rewards: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1.]
Size of the replay buffer: 3, # success: 3
obs torch.Size([3, 96, 96])
prop torch.Size([4])
Warm up done. #episode: 50
#episode from warmup: 47, #reward: 22.0
dict_keys(['obs', 'prop'])
dict_keys(['corner2'])
Saved model to experiments/rl/metaworld/latest_mode_waypoint_boxclose/model0.pt
saved?: True
[5000] Time spent = 1508.14 s
5000: other/elapsed_time  : 1188.53
5000: other/episode       : 51
5000: other/replay        : 101
5000: other/speed         : 4.21
5000: other/step          : 5000
5000: other/train_step    : 2499
5000: score/num_success   : 28
5000: score/score         : 0.00
5000: actor/anorm_bc      [ 4791]: avg:   1.1398, min:   0.5253[ 127], max:   1.8413[3408]
5000: actor/anorm_rl      [ 4791]: avg:   1.4800, min:   0.1613[3434], max:   2.0000[  31]
5000: actor/bc_eval       [ 2000]: avg:   0.2575, min:   0.0000[   1], max:   1.0000[  16]
5000: actor/bc_train      [ 4791]: avg:   0.4310, min:   0.0000[   9], max:   1.0000[   1]
5000: actor/bootstrap_bc  [ 2499]: avg:   0.2672, min:   0.0117[  72], max:   0.7891[1352]
5000: data/batch_R        [ 2499]: avg:   0.0123, min:   0.0000[  56], max:   0.0426[ 117]
5000: data/discount       [ 2499]: avg:   0.9364, min:   0.8945[ 117], max:   0.9665[ 389]
5000: data/episode_len    [   51]: avg:  97.4314, min:  33.0000[  48], max: 100.0000[   1]
5000: data/stddev         [ 4791]: avg:   0.1000, min:   0.1000[  15], max:   0.1000[   2]
5000: score/train_score   [   51]: avg:   0.0588, min:   0.0000[   1], max:   1.0000[  48]
5000: train/actor_loss    [ 2499]: avg:  -0.1901, min:  -0.4411[2369], max:   2.8867[   1]
5000: train/critic_loss   [ 2499]: avg:   0.0210, min:   0.0023[ 570], max:  12.4984[   2]
5000: train/critic_qt     [ 2499]: avg:   0.1675, min:  -0.8647[  11], max:   0.3985[2492]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| env step |  5000 |          75.3 |  33.9 |
| add      |  5000 |           1.2 |   0.6 |
| train    |  2499 |         179.2 |  40.3 |
| act      |  4791 |          36.8 |  15.9 |
| reset    |    51 |         197   |   0.9 |
| eval     |     1 |       94315.3 |   8.5 |
| total(s) |     1 |        1111.3 | 100   |
total time: 0:21:24
Mem info: used: 6.939 GB, avail: 122.854 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/latest_mode_waypoint_boxclose/model0.pt
saved?: True
[10000] Time spent = 1327.06 s
10000: other/elapsed_time  : 1228.78
10000: other/episode       : 121
10000: other/replay        : 171
10000: other/speed         : 4.07
10000: other/step          : 10000
10000: other/train_step    : 4999
10000: score/num_success   : 66
10000: score/score         : 0.05
10000: actor/anorm_bc      [ 4763]: avg:   1.3169, min:   0.5080[ 245], max:   1.8351[3322]
10000: actor/anorm_rl      [ 4763]: avg:   1.2857, min:   0.1864[1276], max:   1.9513[  43]
10000: actor/bc_eval       [ 1944]: avg:   0.3050, min:   0.0000[   1], max:   1.0000[  13]
10000: actor/bc_train      [ 4763]: avg:   0.3359, min:   0.0000[   1], max:   1.0000[   4]
10000: actor/bootstrap_bc  [ 2500]: avg:   0.1876, min:   0.0820[ 273], max:   0.3203[1341]
10000: data/batch_R        [ 2500]: avg:   0.0109, min:   0.0000[   6], max:   0.0465[2491]
10000: data/discount       [ 2500]: avg:   0.9366, min:   0.8869[1884], max:   0.9627[ 115]
10000: data/episode_len    [   70]: avg:  71.7429, min:  32.0000[  49], max: 100.0000[   1]
10000: data/stddev         [ 4763]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[   2]
10000: score/train_score   [   70]: avg:   0.5429, min:   0.0000[   1], max:   1.0000[   2]
10000: train/actor_loss    [ 2500]: avg:  -0.3995, min:  -0.4847[2356], max:  -0.3278[1864]
10000: train/critic_loss   [ 2500]: avg:   0.0083, min:   0.0030[1591], max:   0.0233[ 147]
10000: train/critic_qt     [ 2500]: avg:   0.3876, min:   0.3324[1617], max:   0.4419[2022]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         181.2 |  39.4 |
| act      |  4763 |          37.8 |  15.7 |
| env step |  5000 |          79.3 |  34.5 |
| add      |  5000 |           1.5 |   0.7 |
| reset    |    70 |         218.5 |   1.3 |
| eval     |     1 |       96989.4 |   8.4 |
| total(s) |     1 |        1149.8 | 100   |
total time: 0:43:31
Mem info: used: 6.864 GB, avail: 122.875 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/latest_mode_waypoint_boxclose/model0.pt
saved?: True
[15000] Time spent = 1280.02 s
15000: other/elapsed_time  : 1216.80
15000: other/episode       : 214
15000: other/replay        : 264
15000: other/speed         : 4.11
15000: other/step          : 15000
15000: other/train_step    : 7499
15000: score/num_success   : 134
15000: score/score         : 0.50
15000: actor/anorm_bc      [ 4612]: avg:   1.3045, min:   0.5415[ 496], max:   1.8306[ 161]
15000: actor/anorm_rl      [ 4612]: avg:   1.3948, min:   0.0967[2924], max:   2.0000[ 204]
15000: actor/bc_eval       [ 1310]: avg:   0.1160, min:   0.0000[   1], max:   1.0000[  16]
15000: actor/bc_train      [ 4612]: avg:   0.2496, min:   0.0000[   1], max:   1.0000[   5]
15000: actor/bootstrap_bc  [ 2500]: avg:   0.1669, min:   0.0781[2263], max:   0.2656[  67]
15000: data/batch_R        [ 2500]: avg:   0.0171, min:   0.0000[ 114], max:   0.0580[1698]
15000: data/discount       [ 2500]: avg:   0.9329, min:   0.8604[2283], max:   0.9703[2356]
15000: data/episode_len    [   93]: avg:  53.7849, min:  30.0000[   1], max: 100.0000[   2]
15000: data/stddev         [ 4612]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[   4]
15000: score/train_score   [   93]: avg:   0.7312, min:   0.0000[   2], max:   1.0000[   1]
15000: train/actor_loss    [ 2500]: avg:  -0.4357, min:  -0.5127[2434], max:  -0.3609[ 821]
15000: train/critic_loss   [ 2500]: avg:   0.0077, min:   0.0031[ 118], max:   0.0189[2247]
15000: train/critic_qt     [ 2500]: avg:   0.4246, min:   0.3613[ 803], max:   0.4950[2484]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         181.9 |  41.4 |
| act      |  4612 |          38.1 |  16   |
| env step |  5000 |          75.9 |  34.6 |
| add      |  5000 |           1.8 |   0.8 |
| reset    |    93 |         199.8 |   1.7 |
| eval     |     1 |       61358.7 |   5.6 |
| total(s) |     1 |        1098.9 | 100   |
total time: 1:04:51
Mem info: used: 6.937 GB, avail: 122.785 GB, total: 156.060 GB
saved?: False
[20000] Time spent = 1287.73 s
20000: other/elapsed_time  : 1206.36
20000: other/episode       : 332
20000: other/replay        : 382
20000: other/speed         : 4.14
20000: other/step          : 20000
20000: other/train_step    : 9999
20000: score/num_success   : 239
20000: score/score         : 0.30
20000: actor/anorm_bc      [ 4540]: avg:   1.3524, min:   0.5939[2975], max:   1.8280[1802]
20000: actor/anorm_rl      [ 4540]: avg:   1.4259, min:   0.1067[1821], max:   2.0000[3013]
20000: actor/bc_eval       [ 1587]: avg:   0.1575, min:   0.0000[   1], max:   1.0000[  26]
20000: actor/bc_train      [ 4540]: avg:   0.2249, min:   0.0000[   1], max:   1.0000[  11]
20000: actor/bootstrap_bc  [ 2500]: avg:   0.1215, min:   0.0586[1884], max:   0.1914[ 696]
20000: data/batch_R        [ 2500]: avg:   0.0256, min:   0.0000[ 222], max:   0.0621[1464]
20000: data/discount       [ 2500]: avg:   0.9266, min:   0.8793[1289], max:   0.9665[ 475]
20000: data/episode_len    [  118]: avg:  41.7203, min:  28.0000[  62], max: 100.0000[   7]
20000: data/stddev         [ 4540]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[   4]
20000: score/train_score   [  118]: avg:   0.8898, min:   0.0000[   7], max:   1.0000[   1]
20000: train/actor_loss    [ 2500]: avg:  -0.5372, min:  -0.6554[2190], max:  -0.4180[ 117]
20000: train/critic_loss   [ 2500]: avg:   0.0075, min:   0.0034[2431], max:   0.0237[1365]
20000: train/critic_qt     [ 2500]: avg:   0.5206, min:   0.4280[ 117], max:   0.6141[2190]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         180.6 |  40.8 |
| act      |  4540 |          37.1 |  15.2 |
| env step |  5000 |          74.6 |  33.8 |
| add      |  5000 |           2   |   0.9 |
| reset    |   118 |         182.1 |   1.9 |
| eval     |     1 |       80722.1 |   7.3 |
| total(s) |     1 |        1105.2 | 100   |
total time: 1:26:19
Mem info: used: 6.938 GB, avail: 122.794 GB, total: 156.060 GB
saved?: False
[25000] Time spent = 1277.83 s
25000: other/elapsed_time  : 1197.37
25000: other/episode       : 472
25000: other/replay        : 500
25000: other/speed         : 4.18
25000: other/step          : 25000
25000: other/train_step    : 12499
25000: score/num_success   : 370
25000: score/score         : 0.00
25000: actor/anorm_bc      [ 4459]: avg:   1.3458, min:   0.6120[4103], max:   1.8060[3239]
25000: actor/anorm_rl      [ 4459]: avg:   1.4514, min:   0.0980[2380], max:   2.0000[3611]
25000: actor/bc_eval       [ 2000]: avg:   0.1520, min:   0.0000[   1], max:   1.0000[   6]
25000: actor/bc_train      [ 4459]: avg:   0.2137, min:   0.0000[   1], max:   1.0000[   8]
25000: actor/bootstrap_bc  [ 2500]: avg:   0.1174, min:   0.0586[ 352], max:   0.1875[ 756]
25000: data/batch_R        [ 2500]: avg:   0.0343, min:   0.0000[ 121], max:   0.0813[1753]
25000: data/discount       [ 2500]: avg:   0.9204, min:   0.8680[1892], max:   0.9589[2355]
25000: data/episode_len    [  140]: avg:  36.2786, min:  28.0000[   6], max: 100.0000[   1]
25000: data/stddev         [ 4459]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[  11]
25000: score/train_score   [  140]: avg:   0.9357, min:   0.0000[   1], max:   1.0000[   2]
25000: train/actor_loss    [ 2500]: avg:  -0.6309, min:  -0.7042[2182], max:  -0.5329[  48]
25000: train/critic_loss   [ 2500]: avg:   0.0071, min:   0.0021[2431], max:   0.0204[2113]
25000: train/critic_qt     [ 2500]: avg:   0.6138, min:   0.5305[ 243], max:   0.6811[2011]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         178.1 |  40.6 |
| act      |  4459 |          37.1 |  15.1 |
| env step |  5000 |          73.8 |  33.6 |
| add      |  5000 |           2.3 |   1   |
| reset    |   140 |         193.1 |   2.5 |
| eval     |     1 |       79361.3 |   7.2 |
| total(s) |     1 |        1097.5 | 100   |
total time: 1:47:36
Mem info: used: 6.928 GB, avail: 122.730 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/latest_mode_waypoint_boxclose/model0.pt
saved?: True
[30000] Time spent = 1228.09 s
30000: other/elapsed_time  : 1169.03
30000: other/episode       : 612
30000: other/replay        : 500
30000: other/speed         : 4.28
30000: other/step          : 30000
30000: other/train_step    : 14999
30000: score/num_success   : 503
30000: score/score         : 0.70
30000: actor/anorm_bc      [ 4432]: avg:   1.3434, min:   0.7054[3175], max:   1.8288[ 630]
30000: actor/anorm_rl      [ 4432]: avg:   1.4062, min:   0.2351[1511], max:   1.9492[1552]
30000: actor/bc_eval       [ 1084]: avg:   0.1513, min:   0.0000[   1], max:   1.0000[  23]
30000: actor/bc_train      [ 4432]: avg:   0.2035, min:   0.0000[   1], max:   1.0000[  10]
30000: actor/bootstrap_bc  [ 2500]: avg:   0.1392, min:   0.0703[ 592], max:   0.2383[1735]
30000: data/batch_R        [ 2500]: avg:   0.0526, min:   0.0116[  17], max:   0.1121[1484]
30000: data/discount       [ 2500]: avg:   0.9073, min:   0.8414[1484], max:   0.9513[   2]
30000: data/episode_len    [  140]: avg:  35.5000, min:  27.0000[  26], max: 100.0000[   6]
30000: data/stddev         [ 4432]: avg:   0.1000, min:   0.1000[  25], max:   0.1000[  10]
30000: score/train_score   [  140]: avg:   0.9500, min:   0.0000[   6], max:   1.0000[   1]
30000: train/actor_loss    [ 2500]: avg:  -0.6994, min:  -0.7817[1903], max:  -0.6032[ 139]
30000: train/critic_loss   [ 2500]: avg:   0.0045, min:   0.0012[2127], max:   0.0163[1423]
30000: train/critic_qt     [ 2500]: avg:   0.6889, min:   0.5899[ 390], max:   0.7608[2319]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         171.3 |  40.8 |
| env step |  5000 |          72.6 |  34.6 |
| add      |  5000 |           2.3 |   1.1 |
| act      |  4432 |          37   |  15.6 |
| reset    |   140 |         184   |   2.5 |
| eval     |     1 |       57487.8 |   5.5 |
| total(s) |     1 |        1049.3 | 100   |
total time: 2:08:04
Mem info: used: 6.932 GB, avail: 122.756 GB, total: 156.060 GB
saved?: False
[35000] Time spent = 1295.97 s
35000: other/elapsed_time  : 1207.48
35000: other/episode       : 753
35000: other/replay        : 500
35000: other/speed         : 4.14
35000: other/step          : 35000
35000: other/train_step    : 17499
35000: score/num_success   : 635
35000: score/score         : 0.35
35000: actor/anorm_bc      [ 4510]: avg:   1.3446, min:   0.5847[2597], max:   1.8332[ 872]
35000: actor/anorm_rl      [ 4510]: avg:   1.4204, min:   0.2343[3950], max:   2.0000[3156]
35000: actor/bc_eval       [ 1579]: avg:   0.2958, min:   0.0000[   1], max:   1.0000[  15]
35000: actor/bc_train      [ 4510]: avg:   0.2333, min:   0.0000[   1], max:   1.0000[  73]
35000: actor/bootstrap_bc  [ 2500]: avg:   0.1674, min:   0.0859[ 637], max:   0.2656[1947]
35000: data/batch_R        [ 2500]: avg:   0.0716, min:   0.0193[1705], max:   0.1312[2173]
35000: data/discount       [ 2500]: avg:   0.8940, min:   0.8339[ 874], max:   0.9476[1705]
35000: data/episode_len    [  141]: avg:  35.6312, min:  27.0000[   2], max: 100.0000[   1]
35000: data/stddev         [ 4510]: avg:   0.1000, min:   0.1000[  16], max:   0.1000[   3]
35000: score/train_score   [  141]: avg:   0.9362, min:   0.0000[   1], max:   1.0000[   2]
35000: train/actor_loss    [ 2500]: avg:  -0.7686, min:  -0.8232[1953], max:  -0.6973[  56]
35000: train/critic_loss   [ 2500]: avg:   0.0028, min:   0.0008[1983], max:   0.0135[  60]
35000: train/critic_qt     [ 2500]: avg:   0.7638, min:   0.6982[  62], max:   0.8108[1600]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         178.7 |  40   |
| act      |  4510 |          37.5 |  15.1 |
| env step |  5000 |          74.1 |  33.2 |
| add      |  5000 |           2.5 |   1.1 |
| reset    |   141 |         212.5 |   2.7 |
| eval     |     1 |       87864   |   7.9 |
| total(s) |     1 |        1116.3 | 100   |
total time: 2:29:40
Mem info: used: 6.928 GB, avail: 122.717 GB, total: 156.060 GB
saved?: False
[40000] Time spent = 1325.46 s
40000: other/elapsed_time  : 1231.24
40000: other/episode       : 905
40000: other/replay        : 501
40000: other/speed         : 4.06
40000: other/step          : 40000
40000: other/train_step    : 19999
40000: score/num_success   : 782
40000: score/score         : 0.35
40000: actor/anorm_bc      [ 4484]: avg:   1.3640, min:   0.7222[2803], max:   1.8339[ 350]
40000: actor/anorm_rl      [ 4484]: avg:   1.4757, min:   0.3679[2832], max:   1.9879[1055]
40000: actor/bc_eval       [ 1666]: avg:   0.1387, min:   0.0000[   1], max:   1.0000[  22]
40000: actor/bc_train      [ 4484]: avg:   0.2315, min:   0.0000[   2], max:   1.0000[   1]
40000: actor/bootstrap_bc  [ 2500]: avg:   0.1705, min:   0.0977[1256], max:   0.2656[ 372]
40000: data/batch_R        [ 2500]: avg:   0.0798, min:   0.0194[ 554], max:   0.1549[ 886]
40000: data/discount       [ 2500]: avg:   0.8876, min:   0.8187[ 188], max:   0.9476[ 554]
40000: data/episode_len    [  152]: avg:  32.9671, min:  26.0000[  52], max: 100.0000[  76]
40000: data/stddev         [ 4484]: avg:   0.1000, min:   0.1000[  16], max:   0.1000[   3]
40000: score/train_score   [  152]: avg:   0.9671, min:   0.0000[  76], max:   1.0000[   1]
40000: train/actor_loss    [ 2500]: avg:  -0.7928, min:  -0.8398[1199], max:  -0.7417[  77]
40000: train/critic_loss   [ 2500]: avg:   0.0019, min:   0.0005[2138], max:   0.0100[2440]
40000: train/critic_qt     [ 2500]: avg:   0.7895, min:   0.7416[  77], max:   0.8283[1943]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         186.1 |  40.6 |
| act      |  4484 |          37.3 |  14.6 |
| env step |  5000 |          75.5 |  33   |
| add      |  5000 |           2.6 |   1.1 |
| reset    |   152 |         186.9 |   2.5 |
| eval     |     1 |       93083.6 |   8.1 |
| total(s) |     1 |        1144.5 | 100   |
total time: 2:51:46
Mem info: used: 6.938 GB, avail: 122.701 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/latest_mode_waypoint_boxclose/model0.pt
saved?: True
[45000] Time spent = 1344.58 s
45000: other/elapsed_time  : 1277.43
45000: other/episode       : 1047
45000: other/replay        : 500
45000: other/speed         : 3.91
45000: other/step          : 45000
45000: other/train_step    : 22499
45000: score/num_success   : 916
45000: score/score         : 0.70
45000: actor/anorm_bc      [ 4448]: avg:   1.3357, min:   0.6579[1753], max:   1.8356[ 309]
45000: actor/anorm_rl      [ 4448]: avg:   1.5021, min:   0.3969[ 155], max:   2.0000[ 109]
45000: actor/bc_eval       [ 1370]: avg:   0.2146, min:   0.0000[   1], max:   1.0000[   5]
45000: actor/bc_train      [ 4448]: avg:   0.2111, min:   0.0000[   1], max:   1.0000[  23]
45000: actor/bootstrap_bc  [ 2500]: avg:   0.1732, min:   0.0859[2265], max:   0.2969[ 305]
45000: data/batch_R        [ 2500]: avg:   0.0800, min:   0.0309[ 495], max:   0.1432[ 714]
45000: data/discount       [ 2500]: avg:   0.8874, min:   0.8263[ 714], max:   0.9362[ 312]
45000: data/episode_len    [  142]: avg:  35.1056, min:  24.0000[ 137], max: 100.0000[   3]
45000: data/stddev         [ 4448]: avg:   0.1000, min:   0.1000[   2], max:   0.1000[  18]
45000: score/train_score   [  142]: avg:   0.9437, min:   0.0000[   3], max:   1.0000[   1]
45000: train/actor_loss    [ 2500]: avg:  -0.7941, min:  -0.8372[ 152], max:  -0.7398[ 291]
45000: train/critic_loss   [ 2500]: avg:   0.0022, min:   0.0007[1540], max:   0.0130[ 983]
45000: train/critic_qt     [ 2500]: avg:   0.7913, min:   0.7424[ 585], max:   0.8296[1089]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         191.2 |  41.5 |
| env step |  5000 |          79.2 |  34.4 |
| add      |  5000 |           2.6 |   1.1 |
| act      |  4448 |          38.5 |  14.9 |
| reset    |   142 |         196.9 |   2.4 |
| eval     |     1 |       66147   |   5.7 |
| total(s) |     1 |        1152.3 | 100   |
total time: 3:14:10
Mem info: used: 6.928 GB, avail: 128.814 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/latest_mode_waypoint_boxclose/model0.pt
saved?: True
[50000] Time spent = 1253.46 s
50000: other/elapsed_time  : 1217.49
50000: other/episode       : 1210
50000: other/replay        : 500
50000: other/speed         : 4.11
50000: other/step          : 50000
50000: other/train_step    : 24999
50000: score/num_success   : 1075
50000: score/score         : 0.95
50000: actor/anorm_bc      [ 4440]: avg:   1.3715, min:   0.6921[2144], max:   1.8360[1832]
50000: actor/anorm_rl      [ 4440]: avg:   1.5068, min:   0.3952[3005], max:   2.0000[ 111]
50000: actor/bc_eval       [  641]: avg:   0.1498, min:   0.0000[   1], max:   1.0000[  18]
50000: actor/bc_train      [ 4440]: avg:   0.1687, min:   0.0000[   2], max:   1.0000[   1]
50000: actor/bootstrap_bc  [ 2500]: avg:   0.1349, min:   0.0664[1356], max:   0.2227[ 679]
50000: data/batch_R        [ 2500]: avg:   0.0829, min:   0.0347[1348], max:   0.1433[2211]
50000: data/discount       [ 2500]: avg:   0.8850, min:   0.8187[2211], max:   0.9362[1348]
50000: data/episode_len    [  163]: avg:  30.7485, min:  23.0000[ 147], max: 100.0000[   3]
50000: data/stddev         [ 4440]: avg:   0.1000, min:   0.1000[   6], max:   0.1000[   3]
50000: score/train_score   [  163]: avg:   0.9755, min:   0.0000[   3], max:   1.0000[   1]
50000: train/actor_loss    [ 2500]: avg:  -0.8048, min:  -0.8521[1518], max:  -0.7605[  28]
50000: train/critic_loss   [ 2500]: avg:   0.0014, min:   0.0004[2124], max:   0.0059[ 673]
50000: train/critic_qt     [ 2500]: avg:   0.8010, min:   0.7546[  28], max:   0.8407[2415]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         182.8 |  42.7 |
| act      |  4440 |          35.4 |  14.7 |
| env step |  5000 |          75.9 |  35.5 |
| add      |  5000 |           2.6 |   1.2 |
| reset    |   163 |         176.7 |   2.7 |
| eval     |     1 |       34180.5 |   3.2 |
| total(s) |     1 |        1069.5 | 100   |
total time: 3:35:04
Mem info: used: 6.938 GB, avail: 128.803 GB, total: 156.060 GB
saved?: False
[55000] Time spent = 1239.65 s
55000: other/elapsed_time  : 1194.85
55000: other/episode       : 1372
55000: other/replay        : 501
55000: other/speed         : 4.18
55000: other/step          : 55000
55000: other/train_step    : 27499
55000: score/num_success   : 1233
55000: score/score         : 0.70
55000: actor/anorm_bc      [ 4387]: avg:   1.3594, min:   0.6654[3928], max:   1.8352[ 283]
55000: actor/anorm_rl      [ 4387]: avg:   1.5204, min:   0.4656[3047], max:   2.0000[ 622]
55000: actor/bc_eval       [  972]: avg:   0.1831, min:   0.0000[   1], max:   1.0000[  68]
55000: actor/bc_train      [ 4387]: avg:   0.1637, min:   0.0000[   1], max:   1.0000[   5]
55000: actor/bootstrap_bc  [ 2500]: avg:   0.1228, min:   0.0586[1017], max:   0.2070[2259]
55000: data/batch_R        [ 2500]: avg:   0.0874, min:   0.0386[ 772], max:   0.1663[2190]
55000: data/discount       [ 2500]: avg:   0.8814, min:   0.8035[2190], max:   0.9324[ 772]
55000: data/episode_len    [  162]: avg:  30.8827, min:  24.0000[  71], max: 100.0000[  49]
55000: data/stddev         [ 4387]: avg:   0.1000, min:   0.1000[   6], max:   0.1000[  10]
55000: score/train_score   [  162]: avg:   0.9753, min:   0.0000[  49], max:   1.0000[   1]
55000: train/actor_loss    [ 2500]: avg:  -0.8231, min:  -0.8555[1938], max:  -0.7838[1621]
55000: train/critic_loss   [ 2500]: avg:   0.0011, min:   0.0003[1113], max:   0.0058[1225]
55000: train/critic_qt     [ 2500]: avg:   0.8194, min:   0.7850[  37], max:   0.8470[ 659]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         180.8 |  42.6 |
| act      |  4387 |          35.5 |  14.7 |
| env step |  5000 |          73.3 |  34.5 |
| add      |  5000 |           2.5 |   1.2 |
| reset    |   162 |         185.3 |   2.8 |
| eval     |     1 |       44301.3 |   4.2 |
| total(s) |     1 |        1060.9 | 100   |
total time: 3:55:44
Mem info: used: 6.928 GB, avail: 128.778 GB, total: 156.060 GB
saved?: False
[60000] Time spent = 1269.96 s
60000: other/elapsed_time  : 1216.48
60000: other/episode       : 1534
60000: other/replay        : 500
60000: other/speed         : 4.11
60000: other/step          : 60000
60000: other/train_step    : 29999
60000: score/num_success   : 1388
60000: score/score         : 0.70
60000: actor/anorm_bc      [ 4426]: avg:   1.3509, min:   0.5323[ 972], max:   1.8313[2355]
60000: actor/anorm_rl      [ 4426]: avg:   1.5529, min:   0.4342[1230], max:   2.0000[ 642]
60000: actor/bc_eval       [ 1060]: avg:   0.1500, min:   0.0000[   1], max:   1.0000[  24]
60000: actor/bc_train      [ 4426]: avg:   0.1521, min:   0.0000[   2], max:   1.0000[   1]
60000: actor/bootstrap_bc  [ 2500]: avg:   0.1162, min:   0.0547[1519], max:   0.1953[ 766]
60000: data/batch_R        [ 2500]: avg:   0.0910, min:   0.0386[2393], max:   0.1700[2176]
60000: data/discount       [ 2500]: avg:   0.8781, min:   0.8035[1570], max:   0.9324[2393]
60000: data/episode_len    [  162]: avg:  30.8333, min:  24.0000[  71], max: 100.0000[   7]
60000: data/stddev         [ 4426]: avg:   0.1000, min:   0.1000[  46], max:   0.1000[   6]
60000: score/train_score   [  162]: avg:   0.9568, min:   0.0000[   7], max:   1.0000[   1]
60000: train/actor_loss    [ 2500]: avg:  -0.8353, min:  -0.8696[2488], max:  -0.7985[ 191]
60000: train/critic_loss   [ 2500]: avg:   0.0011, min:   0.0002[1114], max:   0.0074[ 843]
60000: train/critic_qt     [ 2500]: avg:   0.8319, min:   0.7935[   1], max:   0.8642[2095]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         184.3 |  42.3 |
| env step |  5000 |          74.7 |  34.3 |
| add      |  5000 |           2.5 |   1.1 |
| act      |  4426 |          36.1 |  14.7 |
| reset    |   162 |         186.3 |   2.8 |
| eval     |     1 |       52571.7 |   4.8 |
| total(s) |     1 |        1088.9 | 100   |
total time: 4:16:54
Mem info: used: 6.928 GB, avail: 128.759 GB, total: 156.060 GB
