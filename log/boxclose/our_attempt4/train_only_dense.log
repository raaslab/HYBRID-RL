=================config=================
add_bc_loss: 0
batch_size: 256
bc_policy: boxclose
discount: 0.99
env_reward_scale: 1
episode_length: 100
log_per_step: 5000
mix_rl_rate: 1
nstep: 3
num_critic_update: 1
num_eval_episode: 20
num_train_step: 60000
num_warm_up_episode: 50
preload_datapath: release/data/metaworld/BoxClose_frame_stack_1_96x96_end_on_success/dataset.hdf5
preload_num_data: 3
pretrain_epoch_len: 10000
pretrain_num_epoch: 0
q_agent:
  act_method: ibrl
  actor:
    dropout: 0.0
    feature_dim: 64
    hidden_dim: 1024
    max_action_norm: -1
    orth: 1
    spatial_emb: 0
  bc_loss_coef: 0.1
  bc_loss_dynamic: 0
  bootstrap_method: ibrl
  critic:
    drop: 0
    feature_dim: 64
    fuse_patch: 1
    hidden_dim: 1024
    norm_weight: 0
    orth: 1
    spatial_emb: 0
  critic_target_tau: 0.01
  device: cuda
  enc_type: drq
  ibrl_eps_greedy: 1
  lr: 0.0001
  resnet:
    downsample: default
    norm_layer: gnn
    shallow: 0
    stem: default
  resnet96:
    shallow: 0
    use_1x1: 0
  soft_ibrl_beta: 10
  state_actor:
    dropout: 0.5
    hidden_dim: 512
    layer_norm: 0
    num_layer: 3
    orth: 0
  state_critic:
    append_action: 0
    dropout: 0.0
    hidden_dim: 512
    layer_norm: 0
    num_k: 2
    num_layer: 3
    num_q: 10
    orth: 0
  stddev_clip: 0.3
  use_prop: 0
  vit:
    depth: 3
    embed_dim: 128
    embed_norm: 0
    embed_style: embed1
    num_heads: 4
    patch_size: 8
    stride: -1
replay_buffer_size: 500
save_dir: experiments/rl/metaworld/test_boxclose
seed: 1
stddev_max: 0.1
stddev_min: 0.1
stddev_step: 500000
update_freq: 2
use_bc: 1
use_wb: 1
========================================
=========config of loaded agent=========
batchsize: 256
dataset:
  action_repeat: 2
  eval_episode_len: 100
  frame_stack: 1
  max_len: -1
  num_data: 3
  obs_stack: 1
  path: data/metaworld/BoxClose_frame_stack_1_96x96_end_on_success/dataset.hdf5
  rl_camera: corner2
  use_state: 0
ema: -1
epoch_len: 10000
grad_clip: 5
load_model: none
lr: 0.0001
num_epoch: 2
policy:
  dropout: 0
  feature_dim: 256
  hidden_dim: 1024
  net_type: resnet
  orth_init: 1
  proj_dim: 1024
  resnet:
    downsample: default
    norm_layer: gnn
    shallow: 0
    stem: patch
  use_prop: 0
rl_image_size: 96
robot: Sawyer
save_dir: exps/bc/metaworld/pathBoxClose_num_data3_num_epoch2_seed1
seed: 1
task_name: BoxClose
use_wb: 1
weight_decay: 0
========================================
norm layer: gnn
===============Env Config===============
{'action_repeat': 2,
 'camera_names': ['corner2'],
 'device': 'cuda',
 'env_name': 'BoxClose',
 'episode_length': 100,
 'frame_stack': 1,
 'obs_stack': 1,
 'reward_shaping': False,
 'rl_camera': 'corner2',
 'rl_image_size': 96,
 'robots': 'Sawyer',
 'use_state': 0}
========================================
encoder output dim:  39200
patch output dim:  32
============encoder weights=============
DrQEncoder(
  (transform): Resize(size=84, interpolation=bicubic, max_size=None, antialias=True)
  (convnet): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))
    (1): ReLU()
    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
    (3): ReLU()
    (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))
    (7): ReLU()
  )
)
| Module           |   #Params |      % |
|------------------+-----------+--------|
| convnet.0.weight |       864 |   3.02 |
| convnet.0.bias   |        32 |   0.11 |
| convnet.2.weight |     9,216 |  32.18 |
| convnet.2.bias   |        32 |   0.11 |
| convnet.4.weight |     9,216 |  32.18 |
| convnet.4.bias   |        32 |   0.11 |
| convnet.6.weight |     9,216 |  32.18 |
| convnet.6.bias   |        32 |   0.11 |
| Total            |    28,640 | 100.00 |
=============critic weights=============
Critic(
  (q1): _QNet(
    (obs_proj): Sequential(
      (0): Linear(in_features=39200, out_features=64, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
    )
    (q): Sequential(
      (0): Linear(in_features=68, out_features=1024, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (7): ReLU(inplace=True)
      (8): Linear(in_features=1024, out_features=1, bias=True)
    )
  )
  (q2): _QNet(
    (obs_proj): Sequential(
      (0): Linear(in_features=39200, out_features=64, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (3): ReLU()
    )
    (q): Sequential(
      (0): Linear(in_features=68, out_features=1024, bias=True)
      (1): Dropout(p=0, inplace=False)
      (2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (3): ReLU(inplace=True)
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Dropout(p=0, inplace=False)
      (6): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (7): ReLU(inplace=True)
      (8): Linear(in_features=1024, out_features=1, bias=True)
    )
  )
)
| Module               |   #Params |      % |
|----------------------+-----------+--------|
| q1.obs_proj.0.weight | 2,508,800 |  34.51 |
| q1.obs_proj.0.bias   |        64 |   0.00 |
| q1.obs_proj.2.weight |        64 |   0.00 |
| q1.obs_proj.2.bias   |        64 |   0.00 |
| q1.q.0.weight        |    69,632 |   0.96 |
| q1.q.0.bias          |     1,024 |   0.01 |
| q1.q.2.weight        |     1,024 |   0.01 |
| q1.q.2.bias          |     1,024 |   0.01 |
| q1.q.4.weight        | 1,048,576 |  14.43 |
| q1.q.4.bias          |     1,024 |   0.01 |
| q1.q.6.weight        |     1,024 |   0.01 |
| q1.q.6.bias          |     1,024 |   0.01 |
| q1.q.8.weight        |     1,024 |   0.01 |
| q1.q.8.bias          |         1 |   0.00 |
| q2.obs_proj.0.weight | 2,508,800 |  34.51 |
| q2.obs_proj.0.bias   |        64 |   0.00 |
| q2.obs_proj.2.weight |        64 |   0.00 |
| q2.obs_proj.2.bias   |        64 |   0.00 |
| q2.q.0.weight        |    69,632 |   0.96 |
| q2.q.0.bias          |     1,024 |   0.01 |
| q2.q.2.weight        |     1,024 |   0.01 |
| q2.q.2.bias          |     1,024 |   0.01 |
| q2.q.4.weight        | 1,048,576 |  14.43 |
| q2.q.4.bias          |     1,024 |   0.01 |
| q2.q.6.weight        |     1,024 |   0.01 |
| q2.q.6.bias          |     1,024 |   0.01 |
| q2.q.8.weight        |     1,024 |   0.01 |
| q2.q.8.bias          |         1 |   0.00 |
| Total                | 7,268,738 | 100.00 |
=============actor weights==============
Actor(
  (compress): Sequential(
    (0): Linear(in_features=39200, out_features=64, bias=True)
    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.0, inplace=False)
    (3): ReLU()
  )
  (policy): Sequential(
    (0): Linear(in_features=64, out_features=1024, bias=True)
    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.0, inplace=False)
    (3): ReLU()
    (4): Linear(in_features=1024, out_features=1024, bias=True)
    (5): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (6): Dropout(p=0.0, inplace=False)
    (7): ReLU()
    (8): Linear(in_features=1024, out_features=4, bias=True)
    (9): Tanh()
  )
)
| Module            |   #Params |      % |
|-------------------+-----------+--------|
| compress.0.weight | 2,508,800 |  69.05 |
| compress.0.bias   |        64 |   0.00 |
| compress.1.weight |        64 |   0.00 |
| compress.1.bias   |        64 |   0.00 |
| policy.0.weight   |    65,536 |   1.80 |
| policy.0.bias     |     1,024 |   0.03 |
| policy.1.weight   |     1,024 |   0.03 |
| policy.1.bias     |     1,024 |   0.03 |
| policy.4.weight   | 1,048,576 |  28.86 |
| policy.4.bias     |     1,024 |   0.03 |
| policy.5.weight   |     1,024 |   0.03 |
| policy.5.bias     |     1,024 |   0.03 |
| policy.8.weight   |     4,096 |   0.11 |
| policy.8.bias     |         4 |   0.00 |
| Total             | 3,633,348 | 100.00 |
loading first 3 episodes from release/data/metaworld/BoxClose_frame_stack_1_96x96_end_on_success/dataset.hdf5
Raw Dataset size (#episode): 5
rewards: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
rewards: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 1.]
rewards: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 1.]
Size of the replay buffer: 3, # success: 3
obs torch.Size([3, 96, 96])
prop torch.Size([4])
Warm up done. #episode: 50
#episode from warmup: 47, #reward: 21.0
dict_keys(['obs', 'prop'])
dict_keys(['corner2'])
Saved model to experiments/rl/metaworld/test_boxclose/model0.pt
saved?: True
[5000] Time spent = 1236.34 s
5000: other/elapsed_time  : 984.03
5000: other/episode       : 50
5000: other/replay        : 100
5000: other/speed         : 5.08
5000: other/step          : 5000
5000: other/train_step    : 2499
5000: score/num_success   : 24
5000: score/score         : 0.25
5000: actor/anorm_bc      [ 5000]: avg:   1.0907, min:   0.5198[ 267], max:   1.8196[4892]
5000: actor/anorm_rl      [ 5000]: avg:   1.4693, min:   0.2624[4610], max:   2.0000[  28]
5000: actor/bc_eval       [ 1828]: avg:   0.2210, min:   0.0000[   1], max:   1.0000[   3]
5000: actor/bc_train      [ 5000]: avg:   0.3374, min:   0.0000[   1], max:   1.0000[   2]
5000: actor/bootstrap_bc  [ 2499]: avg:   0.1946, min:   0.0000[  46], max:   0.5273[1018]
5000: data/batch_R        [ 2499]: avg:   0.0115, min:   0.0000[ 116], max:   0.0466[ 411]
5000: data/discount       [ 2499]: avg:   0.9368, min:   0.8945[ 187], max:   0.9665[ 308]
5000: data/episode_len    [   50]: avg: 100.0000, min: 100.0000[   1], max: 100.0000[   1]
5000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[  17], max:   0.1000[   2]
5000: score/train_score   [   50]: avg:   0.0000, min:   0.0000[   1], max:   0.0000[   1]
5000: train/actor_loss    [ 2499]: avg:  -0.1867, min:  -0.4685[2397], max:   2.9931[   1]
5000: train/critic_loss   [ 2499]: avg:   0.0209, min:   0.0029[ 452], max:  12.1619[   2]
5000: train/critic_qt     [ 2499]: avg:   0.1534, min:  -0.9119[  12], max:   0.4259[2463]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| act      |  5000 |          29.6 |  15.9 |
| env step |  5000 |          61.6 |  33.2 |
| add      |  5000 |           0.9 |   0.5 |
| train    |  2499 |         155.4 |  41.8 |
| reset    |    50 |         159.2 |   0.9 |
| eval     |     1 |       71944.9 |   7.7 |
| total(s) |     1 |         928.3 | 100   |
total time: 0:17:37
Mem info: used: 6.934 GB, avail: 128.778 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/test_boxclose/model0.pt
saved?: True
[10000] Time spent = 1115.93 s
10000: other/elapsed_time  : 1048.77
10000: other/episode       : 107
10000: other/replay        : 157
10000: other/speed         : 4.77
10000: other/step          : 10000
10000: other/train_step    : 4999
10000: score/num_success   : 42
10000: score/score         : 0.25
10000: actor/anorm_bc      [ 5000]: avg:   1.2316, min:   0.5233[3299], max:   1.8349[ 991]
10000: actor/anorm_rl      [ 5000]: avg:   1.2795, min:   0.1162[1152], max:   1.9775[3302]
10000: actor/bc_eval       [ 1741]: avg:   0.2964, min:   0.0000[   1], max:   1.0000[  13]
10000: actor/bc_train      [ 5000]: avg:   0.3504, min:   0.0000[   1], max:   1.0000[   7]
10000: actor/bootstrap_bc  [ 2500]: avg:   0.2124, min:   0.1016[ 122], max:   0.3320[1250]
10000: data/batch_R        [ 2500]: avg:   0.0080, min:   0.0000[   9], max:   0.0348[ 320]
10000: data/discount       [ 2500]: avg:   0.9378, min:   0.8945[ 241], max:   0.9665[  58]
10000: data/episode_len    [   57]: avg:  87.7018, min:  23.0000[  22], max: 100.0000[   1]
10000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[   2]
10000: score/train_score   [   57]: avg:   0.3158, min:   0.0000[   1], max:   1.0000[   6]
10000: train/actor_loss    [ 2500]: avg:  -0.3810, min:  -0.4952[  16], max:  -0.2812[2241]
10000: train/critic_loss   [ 2500]: avg:   0.0079, min:   0.0030[1842], max:   0.0187[ 301]
10000: train/critic_qt     [ 2500]: avg:   0.3723, min:   0.2966[2439], max:   0.4411[ 380]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         163.2 |  41.8 |
| act      |  5000 |          31.9 |  16.3 |
| env step |  5000 |          65.5 |  33.5 |
| add      |  5000 |           1   |   0.5 |
| reset    |    57 |         193.2 |   1.1 |
| eval     |     1 |       66262.4 |   6.8 |
| total(s) |     1 |         977   | 100   |
total time: 0:36:13
Mem info: used: 6.944 GB, avail: 123.139 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/test_boxclose/model0.pt
saved?: True
[15000] Time spent = 1088.05 s
15000: other/elapsed_time  : 1034.91
15000: other/episode       : 175
15000: other/replay        : 225
15000: other/speed         : 4.83
15000: other/step          : 15000
15000: other/train_step    : 7499
15000: score/num_success   : 83
15000: score/score         : 0.50
15000: actor/anorm_bc      [ 5000]: avg:   1.2547, min:   0.5275[2550], max:   1.8271[ 498]
15000: actor/anorm_rl      [ 5000]: avg:   1.3409, min:   0.3658[ 522], max:   1.9976[ 140]
15000: actor/bc_eval       [ 1518]: avg:   0.2088, min:   0.0000[   1], max:   1.0000[   5]
15000: actor/bc_train      [ 5000]: avg:   0.3190, min:   0.0000[   1], max:   1.0000[   6]
15000: actor/bootstrap_bc  [ 2500]: avg:   0.1876, min:   0.1016[1013], max:   0.2891[ 413]
15000: data/batch_R        [ 2500]: avg:   0.0113, min:   0.0000[   2], max:   0.0386[ 123]
15000: data/discount       [ 2500]: avg:   0.9370, min:   0.8983[ 462], max:   0.9665[2031]
15000: data/episode_len    [   68]: avg:  73.3676, min:  39.0000[  60], max: 100.0000[   2]
15000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[   4]
15000: score/train_score   [   68]: avg:   0.6029, min:   0.0000[   2], max:   1.0000[   1]
15000: train/actor_loss    [ 2500]: avg:  -0.3587, min:  -0.4441[1777], max:  -0.2833[ 292]
15000: train/critic_loss   [ 2500]: avg:   0.0072, min:   0.0030[1865], max:   0.0174[1786]
15000: train/critic_qt     [ 2500]: avg:   0.3502, min:   0.2904[ 245], max:   0.4165[ 918]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         157.4 |  41.4 |
| act      |  5000 |          31.8 |  16.7 |
| env step |  5000 |          65.8 |  34.6 |
| add      |  5000 |           1.2 |   0.6 |
| reset    |    68 |         178.6 |   1.3 |
| eval     |     1 |       51513.5 |   5.4 |
| total(s) |     1 |         951.3 | 100   |
total time: 0:54:21
Mem info: used: 6.932 GB, avail: 122.922 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/test_boxclose/model0.pt
saved?: True
[20000] Time spent = 1086.64 s
20000: other/elapsed_time  : 1034.74
20000: other/episode       : 250
20000: other/replay        : 300
20000: other/speed         : 4.83
20000: other/step          : 20000
20000: other/train_step    : 9999
20000: score/num_success   : 133
20000: score/score         : 0.80
20000: actor/anorm_bc      [ 5000]: avg:   1.2269, min:   0.5094[1613], max:   1.8288[3147]
20000: actor/anorm_rl      [ 5000]: avg:   1.3933, min:   0.3069[ 451], max:   2.0000[ 292]
20000: actor/bc_eval       [ 1084]: avg:   0.2269, min:   0.0000[   3], max:   1.0000[   1]
20000: actor/bc_train      [ 5000]: avg:   0.2328, min:   0.0000[   2], max:   1.0000[   1]
20000: actor/bootstrap_bc  [ 2500]: avg:   0.1662, min:   0.0938[1276], max:   0.2578[1763]
20000: data/batch_R        [ 2500]: avg:   0.0144, min:   0.0000[  12], max:   0.0425[1713]
20000: data/discount       [ 2500]: avg:   0.9351, min:   0.8831[ 835], max:   0.9665[  81]
20000: data/episode_len    [   75]: avg:  66.4533, min:  38.0000[  66], max: 100.0000[   2]
20000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[   4]
20000: score/train_score   [   75]: avg:   0.6667, min:   0.0000[   2], max:   1.0000[   1]
20000: train/actor_loss    [ 2500]: avg:  -0.3827, min:  -0.4650[2383], max:  -0.3116[1224]
20000: train/critic_loss   [ 2500]: avg:   0.0073, min:   0.0031[ 248], max:   0.0179[ 513]
20000: train/critic_qt     [ 2500]: avg:   0.3737, min:   0.3164[1197], max:   0.4417[2423]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         158.9 |  41.8 |
| act      |  5000 |          30.9 |  16.3 |
| env step |  5000 |          65.7 |  34.6 |
| add      |  5000 |           1.3 |   0.7 |
| reset    |    75 |         165.9 |   1.3 |
| eval     |     1 |       50304.9 |   5.3 |
| total(s) |     1 |         949.5 | 100   |
total time: 1:12:27
Mem info: used: 6.932 GB, avail: 122.884 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/test_boxclose/model0.pt
saved?: True
[25000] Time spent = 1058.49 s
25000: other/elapsed_time  : 1018.15
25000: other/episode       : 344
25000: other/replay        : 394
25000: other/speed         : 4.91
25000: other/step          : 25000
25000: other/train_step    : 12499
25000: score/num_success   : 216
25000: score/score         : 0.95
25000: actor/anorm_bc      [ 5000]: avg:   1.2487, min:   0.5625[4484], max:   1.8332[ 660]
25000: actor/anorm_rl      [ 5000]: avg:   1.4059, min:   0.3793[3149], max:   2.0000[ 374]
25000: actor/bc_eval       [  913]: avg:   0.1566, min:   0.0000[   1], max:   1.0000[   2]
25000: actor/bc_train      [ 5000]: avg:   0.1972, min:   0.0000[   1], max:   1.0000[  12]
25000: actor/bootstrap_bc  [ 2500]: avg:   0.1329, min:   0.0430[1460], max:   0.2344[ 793]
25000: data/batch_R        [ 2500]: avg:   0.0192, min:   0.0000[  13], max:   0.0580[2352]
25000: data/discount       [ 2500]: avg:   0.9325, min:   0.8869[ 721], max:   0.9665[1698]
25000: data/episode_len    [   94]: avg:  53.4681, min:  36.0000[  33], max: 100.0000[   4]
25000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[   7], max:   0.1000[  11]
25000: score/train_score   [   94]: avg:   0.8830, min:   0.0000[   4], max:   1.0000[   1]
25000: train/actor_loss    [ 2500]: avg:  -0.4647, min:  -0.5661[2439], max:  -0.3452[  73]
25000: train/critic_loss   [ 2500]: avg:   0.0072, min:   0.0031[ 761], max:   0.0154[1148]
25000: train/critic_qt     [ 2500]: avg:   0.4515, min:   0.3408[  73], max:   0.5428[2451]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         152.4 |  41.3 |
| act      |  5000 |          31.1 |  16.9 |
| env step |  5000 |          64.9 |  35.2 |
| add      |  5000 |           1.3 |   0.7 |
| reset    |    94 |         171.9 |   1.8 |
| eval     |     1 |       38808.7 |   4.2 |
| total(s) |     1 |         922.5 | 100   |
total time: 1:30:06
Mem info: used: 6.923 GB, avail: 122.796 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/test_boxclose/model0.pt
saved?: True
[30000] Time spent = 1090.38 s
30000: other/elapsed_time  : 1047.04
30000: other/episode       : 448
30000: other/replay        : 498
30000: other/speed         : 4.78
30000: other/step          : 30000
30000: other/train_step    : 14999
30000: score/num_success   : 313
30000: score/score         : 0.95
30000: actor/anorm_bc      [ 5000]: avg:   1.2739, min:   0.5033[4593], max:   1.8300[ 672]
30000: actor/anorm_rl      [ 5000]: avg:   1.3977, min:   0.5414[1332], max:   2.0000[1481]
30000: actor/bc_eval       [  903]: avg:   0.1107, min:   0.0000[   1], max:   1.0000[  37]
30000: actor/bc_train      [ 5000]: avg:   0.2294, min:   0.0000[   2], max:   1.0000[   1]
30000: actor/bootstrap_bc  [ 2500]: avg:   0.1205, min:   0.0586[1850], max:   0.2031[1129]
30000: data/batch_R        [ 2500]: avg:   0.0251, min:   0.0000[  78], max:   0.0774[2391]
30000: data/discount       [ 2500]: avg:   0.9287, min:   0.8793[2391], max:   0.9627[ 259]
30000: data/episode_len    [  104]: avg:  48.0962, min:  37.0000[  70], max: 100.0000[  34]
30000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[  26], max:   0.1000[  11]
30000: score/train_score   [  104]: avg:   0.9327, min:   0.0000[  34], max:   1.0000[   1]
30000: train/actor_loss    [ 2500]: avg:  -0.5558, min:  -0.6355[1808], max:  -0.4673[  67]
30000: train/critic_loss   [ 2500]: avg:   0.0063, min:   0.0024[2165], max:   0.0158[1888]
30000: train/critic_qt     [ 2500]: avg:   0.5418, min:   0.4678[  23], max:   0.6252[2457]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         157   |  41.3 |
| act      |  5000 |          31.4 |  16.5 |
| env step |  5000 |          66.6 |  35   |
| add      |  5000 |           1.5 |   0.8 |
| reset    |   104 |         175.9 |   1.9 |
| eval     |     1 |       42161.6 |   4.4 |
| total(s) |     1 |         950.4 | 100   |
total time: 1:48:16
Mem info: used: 6.923 GB, avail: 122.747 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/test_boxclose/model0.pt
saved?: True
[35000] Time spent = 1064.53 s
35000: other/elapsed_time  : 1027.26
35000: other/episode       : 555
35000: other/replay        : 500
35000: other/speed         : 4.87
35000: other/step          : 35000
35000: other/train_step    : 17499
35000: score/num_success   : 412
35000: score/score         : 1.00
35000: actor/anorm_bc      [ 5000]: avg:   1.2569, min:   0.5565[1021], max:   1.8415[1422]
35000: actor/anorm_rl      [ 5000]: avg:   1.4181, min:   0.5976[   2], max:   2.0000[1677]
35000: actor/bc_eval       [  776]: avg:   0.1005, min:   0.0000[   1], max:   1.0000[  27]
35000: actor/bc_train      [ 5000]: avg:   0.1914, min:   0.0000[   1], max:   1.0000[   2]
35000: actor/bootstrap_bc  [ 2500]: avg:   0.1211, min:   0.0625[ 965], max:   0.1914[ 948]
35000: data/batch_R        [ 2500]: avg:   0.0321, min:   0.0000[ 277], max:   0.0851[2335]
35000: data/discount       [ 2500]: avg:   0.9247, min:   0.8718[2335], max:   0.9627[1788]
35000: data/episode_len    [  107]: avg:  46.6822, min:  35.0000[  83], max: 100.0000[  25]
35000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[  16], max:   0.1000[   3]
35000: score/train_score   [  107]: avg:   0.9252, min:   0.0000[  25], max:   1.0000[   1]
35000: train/actor_loss    [ 2500]: avg:  -0.6053, min:  -0.6884[2399], max:  -0.5225[ 775]
35000: train/critic_loss   [ 2500]: avg:   0.0052, min:   0.0021[2027], max:   0.0123[  64]
35000: train/critic_qt     [ 2500]: avg:   0.5944, min:   0.5183[ 268], max:   0.6824[2399]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         153.8 |  41.4 |
| act      |  5000 |          31.4 |  16.9 |
| env step |  5000 |          65.1 |  35.1 |
| add      |  5000 |           1.6 |   0.9 |
| reset    |   107 |         162.8 |   1.9 |
| eval     |     1 |       35711.9 |   3.8 |
| total(s) |     1 |         928   | 100   |
total time: 2:06:01
Mem info: used: 6.932 GB, avail: 122.738 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/test_boxclose/model0.pt
saved?: True
[40000] Time spent = 1067.68 s
40000: other/elapsed_time  : 1030.10
40000: other/episode       : 658
40000: other/replay        : 500
40000: other/speed         : 4.85
40000: other/step          : 40000
40000: other/train_step    : 19999
40000: score/num_success   : 502
40000: score/score         : 1.00
40000: actor/anorm_bc      [ 5000]: avg:   1.2817, min:   0.5593[1453], max:   1.8201[1628]
40000: actor/anorm_rl      [ 5000]: avg:   1.4190, min:   0.2435[3233], max:   2.0000[2440]
40000: actor/bc_eval       [  766]: avg:   0.1018, min:   0.0000[   1], max:   1.0000[   7]
40000: actor/bc_train      [ 5000]: avg:   0.1934, min:   0.0000[   1], max:   1.0000[  14]
40000: actor/bootstrap_bc  [ 2500]: avg:   0.1290, min:   0.0586[ 125], max:   0.2266[1759]
40000: data/batch_R        [ 2500]: avg:   0.0448, min:   0.0115[ 710], max:   0.0967[2476]
40000: data/discount       [ 2500]: avg:   0.9174, min:   0.8604[2476], max:   0.9589[ 710]
40000: data/episode_len    [  103]: avg:  48.4757, min:  35.0000[  30], max: 100.0000[  13]
40000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[  16], max:   0.1000[   3]
40000: score/train_score   [  103]: avg:   0.8738, min:   0.0000[  13], max:   1.0000[   1]
40000: train/actor_loss    [ 2500]: avg:  -0.6772, min:  -0.7407[2385], max:  -0.6008[ 362]
40000: train/critic_loss   [ 2500]: avg:   0.0040, min:   0.0016[2097], max:   0.0167[  28]
40000: train/critic_qt     [ 2500]: avg:   0.6715, min:   0.6027[ 106], max:   0.7305[2255]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         154.5 |  41.5 |
| act      |  5000 |          31.8 |  17.1 |
| env step |  5000 |          64.9 |  34.9 |
| add      |  5000 |           1.5 |   0.8 |
| reset    |   103 |         163.6 |   1.8 |
| eval     |     1 |       36018.2 |   3.9 |
| total(s) |     1 |         930.4 | 100   |
total time: 2:23:48
Mem info: used: 6.923 GB, avail: 122.754 GB, total: 156.060 GB
saved?: False
[45000] Time spent = 1122.92 s
45000: other/elapsed_time  : 1083.59
45000: other/episode       : 775
45000: other/replay        : 500
45000: other/speed         : 4.61
45000: other/step          : 45000
45000: other/train_step    : 22499
45000: score/num_success   : 615
45000: score/score         : 0.95
45000: actor/anorm_bc      [ 5000]: avg:   1.2896, min:   0.5412[4514], max:   1.8286[1623]
45000: actor/anorm_rl      [ 5000]: avg:   1.3928, min:   0.3637[2319], max:   2.0000[ 925]
45000: actor/bc_eval       [  866]: avg:   0.1189, min:   0.0000[   1], max:   1.0000[   5]
45000: actor/bc_train      [ 5000]: avg:   0.2298, min:   0.0000[   1], max:   1.0000[   6]
45000: actor/bootstrap_bc  [ 2500]: avg:   0.1414, min:   0.0664[ 225], max:   0.2305[ 828]
45000: data/batch_R        [ 2500]: avg:   0.0537, min:   0.0115[ 540], max:   0.1161[1620]
45000: data/discount       [ 2500]: avg:   0.9116, min:   0.8566[1086], max:   0.9513[ 125]
45000: data/episode_len    [  117]: avg:  42.7863, min:  32.0000[ 115], max: 100.0000[   5]
45000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[   6], max:   0.1000[   3]
45000: score/train_score   [  117]: avg:   0.9658, min:   0.0000[   5], max:   1.0000[   1]
45000: train/actor_loss    [ 2500]: avg:  -0.7138, min:  -0.7812[2329], max:  -0.6428[ 257]
45000: train/critic_loss   [ 2500]: avg:   0.0030, min:   0.0009[2457], max:   0.0149[1400]
45000: train/critic_qt     [ 2500]: avg:   0.7105, min:   0.6348[ 434], max:   0.7679[2344]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         161.7 |  41.3 |
| act      |  5000 |          33.8 |  17.3 |
| env step |  5000 |          67.6 |  34.5 |
| add      |  5000 |           1.8 |   0.9 |
| reset    |   117 |         180   |   2.2 |
| eval     |     1 |       38383   |   3.9 |
| total(s) |     1 |         979.5 | 100   |
total time: 2:42:31
Mem info: used: 6.923 GB, avail: 122.729 GB, total: 156.060 GB
saved?: False
[50000] Time spent = 1071.10 s
50000: other/elapsed_time  : 1035.04
50000: other/episode       : 881
50000: other/replay        : 500
50000: other/speed         : 4.83
50000: other/step          : 50000
50000: other/train_step    : 24999
50000: score/num_success   : 708
50000: score/score         : 0.90
50000: actor/anorm_bc      [ 5000]: avg:   1.2537, min:   0.5401[ 545], max:   1.8395[2868]
50000: actor/anorm_rl      [ 5000]: avg:   1.4051, min:   0.3245[1154], max:   2.0000[ 772]
50000: actor/bc_eval       [  891]: avg:   0.1470, min:   0.0000[   1], max:   1.0000[  24]
50000: actor/bc_train      [ 5000]: avg:   0.2080, min:   0.0000[   1], max:   1.0000[  12]
50000: actor/bootstrap_bc  [ 2500]: avg:   0.1430, min:   0.0703[ 561], max:   0.2266[1838]
50000: data/batch_R        [ 2500]: avg:   0.0594, min:   0.0116[1408], max:   0.1121[1171]
50000: data/discount       [ 2500]: avg:   0.9073, min:   0.8490[1267], max:   0.9551[1408]
50000: data/episode_len    [  106]: avg:  47.1792, min:  28.0000[  36], max: 100.0000[  25]
50000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[   6], max:   0.1000[   3]
50000: score/train_score   [  106]: avg:   0.8774, min:   0.0000[  25], max:   1.0000[   1]
50000: train/actor_loss    [ 2500]: avg:  -0.7509, min:  -0.7973[1314], max:  -0.7042[ 519]
50000: train/critic_loss   [ 2500]: avg:   0.0020, min:   0.0007[ 527], max:   0.0115[1753]
50000: train/critic_qt     [ 2500]: avg:   0.7475, min:   0.7035[ 519], max:   0.7910[ 889]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         154.2 |  41.2 |
| act      |  5000 |          31.8 |  17   |
| env step |  5000 |          66   |  35.3 |
| add      |  5000 |           1.4 |   0.8 |
| reset    |   106 |         170.1 |   1.9 |
| eval     |     1 |       35505.2 |   3.8 |
| total(s) |     1 |         935.1 | 100   |
total time: 3:00:23
Mem info: used: 6.932 GB, avail: 122.734 GB, total: 156.060 GB
saved?: False
[55000] Time spent = 1050.42 s
55000: other/elapsed_time  : 1008.99
55000: other/episode       : 992
55000: other/replay        : 500
55000: other/speed         : 4.96
55000: other/step          : 55000
55000: other/train_step    : 27499
55000: score/num_success   : 809
55000: score/score         : 0.95
55000: actor/anorm_bc      [ 5000]: avg:   1.2762, min:   0.5583[3923], max:   1.8290[3311]
55000: actor/anorm_rl      [ 5000]: avg:   1.3719, min:   0.1621[1455], max:   2.0000[1533]
55000: actor/bc_eval       [  874]: avg:   0.1030, min:   0.0000[   1], max:   1.0000[  22]
55000: actor/bc_train      [ 5000]: avg:   0.1978, min:   0.0000[   1], max:   1.0000[  10]
55000: actor/bootstrap_bc  [ 2500]: avg:   0.1475, min:   0.0703[ 905], max:   0.2188[1151]
55000: data/batch_R        [ 2500]: avg:   0.0586, min:   0.0115[1941], max:   0.1124[2233]
55000: data/discount       [ 2500]: avg:   0.9073, min:   0.8528[ 922], max:   0.9513[1941]
55000: data/episode_len    [  111]: avg:  44.8378, min:  29.0000[  51], max: 100.0000[  27]
55000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[   6], max:   0.1000[  10]
55000: score/train_score   [  111]: avg:   0.9099, min:   0.0000[  27], max:   1.0000[   1]
55000: train/actor_loss    [ 2500]: avg:  -0.7341, min:  -0.7822[1630], max:  -0.6820[ 319]
55000: train/critic_loss   [ 2500]: avg:   0.0024, min:   0.0009[1956], max:   0.0132[ 887]
55000: train/critic_qt     [ 2500]: avg:   0.7316, min:   0.6819[1910], max:   0.7812[ 763]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         152.9 |  41.7 |
| act      |  5000 |          30.8 |  16.8 |
| env step |  5000 |          63   |  34.4 |
| add      |  5000 |           1.4 |   0.8 |
| reset    |   111 |         148.9 |   1.8 |
| eval     |     1 |       40680   |   4.4 |
| total(s) |     1 |         915.9 | 100   |
total time: 3:17:53
Mem info: used: 6.932 GB, avail: 122.757 GB, total: 156.060 GB
Saved model to experiments/rl/metaworld/test_boxclose/model0.pt
saved?: True
[60000] Time spent = 1193.64 s
60000: other/elapsed_time  : 1157.22
60000: other/episode       : 1116
60000: other/replay        : 500
60000: other/speed         : 4.32
60000: other/step          : 60000
60000: other/train_step    : 29999
60000: score/num_success   : 924
60000: score/score         : 1.00
60000: actor/anorm_bc      [ 5000]: avg:   1.2857, min:   0.5289[3778], max:   1.8266[4483]
60000: actor/anorm_rl      [ 5000]: avg:   1.3824, min:   0.3525[2295], max:   1.9970[2992]
60000: actor/bc_eval       [  648]: avg:   0.1528, min:   0.0000[   2], max:   1.0000[   1]
60000: actor/bc_train      [ 5000]: avg:   0.1992, min:   0.0000[   1], max:   1.0000[   5]
60000: actor/bootstrap_bc  [ 2500]: avg:   0.1412, min:   0.0742[1823], max:   0.2266[2295]
60000: data/batch_R        [ 2500]: avg:   0.0592, min:   0.0155[1601], max:   0.1120[2227]
60000: data/discount       [ 2500]: avg:   0.9065, min:   0.8528[1213], max:   0.9513[1601]
60000: data/episode_len    [  124]: avg:  40.4839, min:  26.0000[  74], max: 100.0000[  26]
60000: data/stddev         [ 5000]: avg:   0.1000, min:   0.1000[  35], max:   0.1000[  10]
60000: score/train_score   [  124]: avg:   0.9274, min:   0.0000[  26], max:   1.0000[   1]
60000: train/actor_loss    [ 2500]: avg:  -0.7420, min:  -0.7858[2313], max:  -0.6959[1248]
60000: train/critic_loss   [ 2500]: avg:   0.0019, min:   0.0007[1188], max:   0.0068[1797]
60000: train/critic_qt     [ 2500]: avg:   0.7385, min:   0.6947[2030], max:   0.7834[2349]
Timer Info:
| name     |   num |   t/call (ms) |     % |
|----------+-------+---------------+-------|
| train    |  2500 |         171.8 |  41.3 |
| act      |  5000 |          34.8 |  16.8 |
| env step |  5000 |          73.6 |  35.4 |
| add      |  5000 |           1.9 |   0.9 |
| reset    |   124 |         189.4 |   2.3 |
| eval     |     1 |       34409.3 |   3.3 |
| total(s) |     1 |        1039.3 | 100   |
total time: 3:37:47
Mem info: used: 6.932 GB, avail: 122.695 GB, total: 156.060 GB
